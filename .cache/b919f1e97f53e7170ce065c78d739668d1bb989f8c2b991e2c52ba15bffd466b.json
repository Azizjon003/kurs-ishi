{
  "url": "https://medium.com/@luiz.niero/ai-fundamentals-3-some-concepts-and-definitions-b6020950f949",
  "markdown": "# AI Fundamentals #3 â€” Some concepts and definitions\n*By Luiz Henrique Pereira Niero*\n---\n[\n![Luiz Henrique Pereira Niero](https://miro.medium.com/v2/da:true/resize:fill:64:64/0*zrlYe9xRSqXFWwV6)\n](https://medium.com/@luiz.niero?source=post_page---byline--b6020950f949---------------------------------------)\n6 min read\nApr 25, 2025\n\\--\nThis is the third article in the **AI Fundamentals** series. If you havenâ€™t read the first two yet, I \\*highly recommend\\* checking them out real quick:\n1 â€” \\[https://medium.com/@luiz.niero/ai-fundamentals-the-perceptron-ca2e68705f1f\\]\n2 â€” \\[https://medium.com/@luiz.niero/ai-fundamentals-activation-functions-93c0f9c687af\\]\nThis time, weâ€™re gonna take a look at the definitions and building blocks of **Artificial Neural Networks** (ANNs). I know this post might not be as smooth to read as the others, but trust me â€” knowing the parts of a neural network is really important.\nâ€” -\n### 1\\. Architecture-related definitions\nIf youâ€™re into AI or have read anything about it before, youâ€™ve probably heard of \\*layers\\*. But what are they, exactly? A layer is just a group of neurons that are on the same level of depth in the network. If you look at a typical diagram of a neural network (like the one below), each c**olumn of circles** represents a layer of neurons. Youâ€™ll get it in a second!\n-   **Neuron:** Think of it as the atom of a neural network. Each neuron takes in some value â€” either from the input vector (if itâ€™s an input neuron) or from the neuron before it. Each input value has a **weight** attached to it. Neurons are always linked to an **activation function,** which decides whether and how the neuronâ€™s output is passed forward.\n-   **Number of layers:** Thatâ€™s how many layers of neurons the network has. Technically, the minimum is 1, but that doesnâ€™t do much. The real minimum that works is 2, and weâ€™ve already seen this setup in the\n-   **Perceptron** (check article 1 if you havenâ€™t yet â€” donâ€™t be stubborn! haha).\n    The number of layers affects how well a network can model a problem. Itâ€™s closely tied to how good the model is at learning patterns, but also how likely it is to **overfit**. Iâ€™ll write a full article about network architecture soon, so stay tuned!\n-   **Number of nodes per layer**: Each layer has a number of **nodes**\\* and each one is a neuron. Depending on the problem, weâ€™ll have 1 or more neurons in the output layer. Usually, the output layer has fewer neurons, but there are exceptions where it has the same number as other layers. Like the number of layers, the number of neurons per layer also affects performance, overfitting, and learning. Again â€” more on this when we talk about architecture!\n-   **Hidden layers**: These are the layers between the input and output layers. They donâ€™t receive raw input or give final results â€” they just process stuff in between. So, if a network has 4 hidden layers, it actually has 6 in total: 1 input, 4 hidden, and 1 output.\nIn the diagram, each circle is a neuron. The input layer doesnâ€™t have neurons â€” itâ€™s just the raw data from the sample.\nPress enter or click to view image in full size\nâ€” -\n### 2\\. What makes a network learn\nBy definition, an artificial neural network is a **universal approximator**. In simple terms, it can learn â€œanyâ€ mathematical function if you give it enough time and data. Learning involves a few key parts:\n-   **Weight**: A real number that shows how much a certain neuron influences the final output. If the weight is big, that neuron plays a big role. If itâ€™s close to zero, it barely matters. We usually use \\`wáµ¢â±¼\\` to represent weights, and \\`W\\` for the full weight matrix of a layer.\n-   **Learning rate**: A real number that tells how much a weight should change during training. Common values are between 0.01 and 0.0001. If the rateâ€™s too high, the model jumps around too much and never settles. If itâ€™s too low, training is super slow or might not work at all.\n    In the Github repo for the Perceptron article, you can see how smaller learning rates make the error graph smoother.\n-   **Optimizers**: Sometimes the network gets stuck in a local minimum (a suboptimal solution). Optimizers are strategies that help avoid this, either by adding randomness or changing the learning logic. Itâ€™s a whole topic by itself â€” I promise to write an article just on this soon!\n-   **Acceptable minimum error**: Itâ€™s a good idea to tell the model what level of error is â€œgood enoughâ€ so it can stop training when it gets there. In real-world problems, error is almost never zero. For example, if youâ€™re okay with 95% accuracy, you set the acceptable error to 0.05.\n-   **Online learning**: In this method, the weights are updated after each sample. It usually needs fewer training epochs and works well for real-time data or when computing resources are limited.\n-   **Offline learning**: Here, weights are updated only after a bunch of samples. You accumulate error and adjust the weights after processing a batch. Itâ€™s more stable but uses more resources.\n-   **Batch**: In offline learning, if you donâ€™t want to use all samples at once (maybe your machine canâ€™t handle it), you define a batch size â€” like 32 samples per step. This is called \\`batch\\_size\\`.\n-   **Full Batch**: When you process the entire dataset before adjusting the weights. In this case, \\`batch\\_size = number of samples\\`.\n-   **Underfitting**: This means the model didnâ€™t learn enough and performs poorly. Could be due to bad data, unlucky weight initialization, poor architecture, or bad learning rate. Youâ€™ll know this is happening if the error doesnâ€™t drop across epochs.\nPress enter or click to view image in full size\n-   Overfitting: This happens when the network â€œmemorizesâ€ the training data instead of learning patterns. It usually occurs after too many training epochs. Youâ€™ll notice this when the training error keeps going down, but the test error starts going up.\nPress enter or click to view image in full size\nâ€” -\n### 3\\. Types of Learning\nI could have added this to the last section, but I think it deserves its own. There are different ways to teach a neural network depending on the problem.\n-   **Supervised Learning**: You teach the network using labeled data. For example: 100 images of fruits labeled as â€œorange,â€ â€œbanana,â€ or â€œwatermelon.â€ The input is the pixel matrix, and the model tries to guess the fruit. The network adjusts its weights to reduce errors.\n    The dataset used for training is called the **Training Set**; the one used to evaluate is the **Test Set**. This is great for classification or regression problems.\n-   **Unsupervised Learning**: Here, there are no labels. The network tries to find patterns or group similar data. Itâ€™s often used for analyzing datasets.\n    Example: discovering how many different customer profiles exist in a service. People with similar behavior tend to form natural groups.\n-   **Self-supervised Learning**: In this method, the model learns from the data by generating its own labels. Itâ€™s heavily used in NLP (Natural Language Processing) tasks, especially in Transformers â€” the architecture behind most LLMs (like ChatGPT).\nâ€” -\nWell, I think Iâ€™m still far from covering all the terms in the world of neural networks, but this article is already pretty long! Iâ€™ll explain more specific concepts as this series gets deeper.\nThanks for reading! And if you spot any mistakes, feel free to ping me on LinkedIn. Appreciate it! ğŸ˜Š\n**About me:** Iâ€™m a computer science graduate and MSc from UNESP, currently working as a senior developer and data scientist in a healthtech startup. Iâ€™m also prepping for my PhD and love teaching AI in a simple and accessible way â€” through blog posts, talks, or just a good olâ€™ coffee chat â˜•ğŸ’». In: [https://www.linkedin.com/in/luiz-niero-msc/](https://www.linkedin.com/in/luiz-niero-msc/)",
  "timestamp": 1762690819501,
  "title": "AI Fundamentals #3 â€” Some concepts and definitions"
}