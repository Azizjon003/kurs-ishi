{
  "url": "https://ml-course.github.io/master/notebooks/06%20-%20Neural%20Networks.html",
  "markdown": "# Lecture 6. Neural Networks — ML Engineering\n[Skip to main content](#main-content)\nBack to top Ctrl+K\n [![ML Engineering - Home](https://ml-course.github.io/master/_static/banner.jpeg) ![ML Engineering - Home](https://ml-course.github.io/master/_static/banner.jpeg)](https://ml-course.github.io/master/intro.html)\nSearch Ctrl+K\n-   [Welcome](https://ml-course.github.io/master/intro.html)\n-   [Prerequisites](https://ml-course.github.io/master/labs/Lab%200%20-%20Prerequisites.html)\nLectures\n-   [Lecture 1. Introduction](https://ml-course.github.io/master/notebooks/01%20-%20Introduction.html)\n-   [Lecture 2. Linear models](https://ml-course.github.io/master/notebooks/02%20-%20Linear%20Models.html)\n-   [Lecture 3. Model Evaluation](https://ml-course.github.io/master/notebooks/03%20-%20Model%20Evaluation.html)\n-   [Lecture 4. Ensemble Learning](https://ml-course.github.io/master/notebooks/04%20-%20Ensemble%20Learning.html)\n-   [Lecture 5. Data preprocessing](https://ml-course.github.io/master/notebooks/05%20-%20Data%20Preprocessing.html)\n-   [Lecture 6. Neural Networks](#)\n-   [Lecture 7: Convolutional Neural Networks](https://ml-course.github.io/master/notebooks/07%20-%20Convolutional%20Neural%20Networks.html)\n-   [Lecture 8. Transformers](https://ml-course.github.io/master/notebooks/08%20-%20Transformers.html)\nLabs\n-   [Lab 1a: Linear regression](https://ml-course.github.io/master/labs/Lab%201a%20-%20Linear%20Models%20for%20Regression.html)\n-   [Lab 1b: Linear classification](https://ml-course.github.io/master/labs/Lab%201b%20-%20Linear%20Models%20for%20Classification.html)\n-   [Lab 3: Ensembles](https://ml-course.github.io/master/labs/Lab%203a%20-%20Ensembles.html)\n-   [Lab 4: Data preprocessing](https://ml-course.github.io/master/labs/Lab%203b%20-%20Pipelines.html)\n-   [Lab 4: Neural networks](https://ml-course.github.io/master/labs/Lab%204%20-%20Neural%20Networks.html)\n-   [Lab 7a: Convolutional neural nets](https://ml-course.github.io/master/labs/Lab%205%20-%20Convolutional%20Neural%20Networks.html)\nTutorials\n-   [Python for data analysis](https://ml-course.github.io/master/notebooks/Tutorial%201%20-%20Python.html)\n-   [Python for scientific computing](https://ml-course.github.io/master/notebooks/Tutorial%202%20-%20Python%20for%20Data%20Analysis.html)\n-   [Machine Learning in Python](https://ml-course.github.io/master/notebooks/Tutorial%203%20-%20Machine%20Learning%20in%20Python.html)\n-   [Lab 1: Machine Learning with Python](https://ml-course.github.io/master/labs/Lab%201%20-%20Tutorial.html)\n-   [Lab 2: Model Selection in scikit-learn](https://ml-course.github.io/master/labs/Lab%202%20-%20Tutorial.html)\n-   [Lab 4: Data engineering pipelines with scikit-learn](https://ml-course.github.io/master/labs/Lab%203%20-%20Tutorial.html)\n-   [Lab 4: Deep Learning with PyTorch](https://ml-course.github.io/master/labs/Lab%204%20-%20Tutorial.html)\n-    [![Colab logo](https://ml-course.github.io/master/_static/images/logo_colab.png)Colab](https://colab.research.google.com/github/ml-course/master/blob/master/notebooks/06 - Neural Networks.ipynb \"Launch on Colab\")\n-   [Repository](https://github.com/ml-course/master \"Source repository\")\n-   [Open issue](https://github.com/ml-course/master/issues/new?title=Issue%20on%20page%20%2Fnotebooks/06 - Neural Networks.html&body=Your%20issue%20content%20here. \"Open an issue\")\n-   [.ipynb](https://ml-course.github.io/master/_sources/notebooks/06%20-%20Neural%20Networks.ipynb \"Download source file\")\n-   .pdf\n# Lecture 6. Neural Networks\n## Contents\n-   [Overview](#overview)\n-   [Architecture](#architecture)\n    -   [Basic Architecture](#basic-architecture)\n    -   [More layers](#more-layers)\n    -   [Why layers?](#why-layers)\n    -   [Other architectures](#other-architectures)\n-   [Training Neural Nets](#training-neural-nets)\n    -   [Mini-batch Stochastic Gradient Descent (recap)](#mini-batch-stochastic-gradient-descent-recap)\n    -   [Forward pass](#forward-pass)\n        -   [Tensor operations](#tensor-operations)\n        -   [Element-wise operations](#element-wise-operations)\n    -   [Backward pass (backpropagation)](#backward-pass-backpropagation)\n        -   [Example](#example)\n        -   [Backpropagation (2)](#backpropagation-2)\n        -   [Backpropagation (3)](#backpropagation-3)\n        -   [Summary](#summary)\n-   [Activation functions for hidden layers](#activation-functions-for-hidden-layers)\n    -   [Effect of activation functions on the gradient](#effect-of-activation-functions-on-the-gradient)\n    -   [ReLU vs Tanh](#relu-vs-tanh)\n    -   [Activation functions for output layer](#activation-functions-for-output-layer)\n-   [Weight initialization](#weight-initialization)\n    -   [Weight initialization: transfer learning](#weight-initialization-transfer-learning)\n-   [Optimizers](#optimizers)\n    -   [SGD with learning rate schedules](#sgd-with-learning-rate-schedules)\n    -   [SGD with learning rate schedules](#id1)\n    -   [Momentum](#momentum)\n        -   [Momentum in practice](#momentum-in-practice)\n    -   [Adaptive gradients](#adaptive-gradients)\n    -   [Adam (Adaptive moment estimation)](#adam-adaptive-moment-estimation)\n    -   [SGD Optimizer Zoo](#sgd-optimizer-zoo)\n-   [Neural networks in practice](#neural-networks-in-practice)\n    -   [Preparing the data](#preparing-the-data)\n    -   [Building the network](#building-the-network)\n    -   [Choosing loss, optimizer, metrics](#choosing-loss-optimizer-metrics)\n    -   [Training on GPU](#training-on-gpu)\n    -   [Training loop](#training-loop)\n    -   [`loss.backward()`](#loss-backward)\n    -   [In PyTorch Lightning](#in-pytorch-lightning)\n    -   [Training](#training)\n    -   [Lightning Trainer](#lightning-trainer)\n        -   [Choosing training hyperparameters](#choosing-training-hyperparameters)\n-   [Model selection](#model-selection)\n    -   [Early stopping](#early-stopping)\n    -   [Regularization and memorization capacity](#regularization-and-memorization-capacity)\n        -   [Weight regularization (weight decay)](#weight-regularization-weight-decay)\n    -   [Dropout](#dropout)\n        -   [Dropout layers](#dropout-layers)\n        -   [Batch Normalization](#batch-normalization)\n        -   [BatchNorm layers](#batchnorm-layers)\n    -   [New model](#new-model)\n    -   [New model (Sequential API)](#new-model-sequential-api)\n    -   [Other logging tools](#other-logging-tools)\n-   [Summary](#id2)\n# Lecture 6. Neural Networks[#](#lecture-6-neural-networks \"Link to this heading\")\n**How to train your neurons**\nJoaquin Vanschoren\nShow code cell source Hide code cell source\n\\# Auto-setup when running on Google Colab\nimport os\nif 'google.colab' in str(get\\_ipython()) and not os.path.exists('/content/master'):\n    !git clone \\-q https://github.com/ML-course/master.git /content/master\n    !pip \\--quiet install \\-r /content/master/requirements\\_colab.txt\n    %cd master/notebooks\n\\# Global imports and settings\n%matplotlib inline\nfrom preamble import \\*\ninteractive \\= False \\# Set to True for interactive plots\nif interactive:\n    fig\\_scale \\= 0.5\n    plt.rcParams.update(print\\_config)\nelse: \\# For printing\n    fig\\_scale \\= 0.4\n    plt.rcParams.update(print\\_config)\n#HTML('''<style>.reveal pre code {max-height: 1000px !important;}</style>''')\n## Overview[#](#overview \"Link to this heading\")\n-   Neural architectures\n-   Training neural nets\n    -   Forward pass: Tensor operations\n    -   Backward pass: Backpropagation\n-   Neural network design:\n    -   Activation functions\n    -   Weight initialization\n    -   Optimizers\n-   Neural networks in practice\n-   Model selection\n    -   Early stopping\n    -   Memorization capacity and information bottleneck\n    -   L1/L2 regularization\n    -   Dropout\n    -   Batch normalization\nShow code cell source Hide code cell source\ndef draw\\_neural\\_net(ax, layer\\_sizes, draw\\_bias\\=False, labels\\=False, activation\\=False, sigmoid\\=False,\n                    weight\\_count\\=False, random\\_weights\\=False, show\\_activations\\=False, figsize\\=(4, 4)):\n    \"\"\"\n    Draws a dense neural net for educational purposes\n    Parameters:\n        ax: plot axis\n        layer\\_sizes: array with the sizes of every layer\n        draw\\_bias: whether to draw bias nodes\n        labels: whether to draw labels for the weights and nodes\n        activation: whether to show the activation function inside the nodes\n        sigmoid: whether the last activation function is a sigmoid\n        weight\\_count: whether to show the number of weights and biases\n        random\\_weights: whether to show random weights as colored lines\n        show\\_activations: whether to show a variable for the node activations\n        scale\\_ratio: ratio of the plot dimensions, e.g. 3/4\n    \"\"\"\n    figsize \\= (figsize\\[0\\]\\*fig\\_scale, figsize\\[1\\]\\*fig\\_scale)\n    left, right, bottom, top \\= 0.1, 0.89\\*figsize\\[0\\]/figsize\\[1\\], 0.1, 0.89\n    n\\_layers \\= len(layer\\_sizes)\n    v\\_spacing \\= (top \\- bottom)/float(max(layer\\_sizes))\n    h\\_spacing \\= (right \\- left)/float(len(layer\\_sizes) \\- 1)\n    colors \\= \\['greenyellow','cornflowerblue','lightcoral'\\]\n    w\\_count, b\\_count \\= 0, 0\n    ax.set\\_xlim(0, figsize\\[0\\]/figsize\\[1\\])\n    ax.axis('off')\n    ax.set\\_aspect('equal')\n    txtargs \\= {\"fontsize\":12\\*fig\\_scale, \"verticalalignment\":'center', \"horizontalalignment\":'center', \"zorder\":5}\n    \\# Draw biases by adding a node to every layer except the last one\n    if draw\\_bias:\n        layer\\_sizes \\= \\[x+1 for x in layer\\_sizes\\]\n        layer\\_sizes\\[\\-1\\] \\= layer\\_sizes\\[\\-1\\] \\- 1\n    \\# Nodes\n    for n, layer\\_size in enumerate(layer\\_sizes):\n        layer\\_top \\= v\\_spacing\\*(layer\\_size \\- 1)/2. + (top + bottom)/2.\n        node\\_size \\= v\\_spacing/len(layer\\_sizes) if activation and n!=0 else v\\_spacing/3.\n        if n\\==0:\n            color \\= colors\\[0\\]\n        elif n\\==len(layer\\_sizes)\\-1:\n            color \\= colors\\[2\\]\n        else:\n            color \\= colors\\[1\\]\n        for m in range(layer\\_size):\n            ax.add\\_artist(plt.Circle((n\\*h\\_spacing + left, layer\\_top \\- m\\*v\\_spacing), radius\\=node\\_size,\n                                      color\\=color, ec\\='k', zorder\\=4, linewidth\\=fig\\_scale))\n            b\\_count += 1\n            nx, ny \\= n\\*h\\_spacing + left, layer\\_top \\- m\\*v\\_spacing\n            nsx, nsy \\= \\[n\\*h\\_spacing + left,n\\*h\\_spacing + left\\], \\[layer\\_top \\- m\\*v\\_spacing \\- 0.5\\*node\\_size\\*2,layer\\_top \\- m\\*v\\_spacing + 0.5\\*node\\_size\\*2\\]\n            if draw\\_bias and m\\==0 and n<len(layer\\_sizes)\\-1:\n                ax.text(nx, ny, r'$1$', \\*\\*txtargs)\n            elif labels and n\\==0:\n                ax.text(n\\*h\\_spacing + left,layer\\_top + v\\_spacing/1.5, 'input', \\*\\*txtargs)\n                ax.text(nx, ny, r'$x\\_{}$'.format(m), \\*\\*txtargs)\n            elif labels and n\\==len(layer\\_sizes)\\-1:\n                if activation:\n                    if sigmoid:\n                        ax.text(n\\*h\\_spacing + left,layer\\_top \\- m\\*v\\_spacing, r\"$z \\\\;\\\\;\\\\; \\\\sigma$\", \\*\\*txtargs)\n                    else:\n                        ax.text(n\\*h\\_spacing + left,layer\\_top \\- m\\*v\\_spacing, r\"$z\\_{} \\\\;\\\\; g$\".format(m), \\*\\*txtargs)\n                    ax.add\\_artist(plt.Line2D(nsx, nsy, c\\='k', zorder\\=6))\n                    if show\\_activations:\n                        ax.text(n\\*h\\_spacing + left + 1.5\\*node\\_size,layer\\_top \\- m\\*v\\_spacing, r\"$\\\\hat{y}$\", fontsize\\=12\\*fig\\_scale,\n                                verticalalignment\\='center', horizontalalignment\\='left', zorder\\=5, c\\='r')\n                else:\n                    ax.text(nx, ny, r'$o\\_{}$'.format(m), \\*\\*txtargs)\n                ax.text(n\\*h\\_spacing + left,layer\\_top + v\\_spacing, 'output', \\*\\*txtargs)\n            elif labels:\n                if activation:\n                    ax.text(n\\*h\\_spacing + left,layer\\_top \\- m\\*v\\_spacing, r\"$z\\_{} \\\\;\\\\; f$\".format(m), \\*\\*txtargs)\n                    ax.add\\_artist(plt.Line2D(nsx, nsy, c\\='k', zorder\\=6))\n                    if show\\_activations:\n                        ax.text(n\\*h\\_spacing + left + node\\_size\\*1.2 ,layer\\_top \\- m\\*v\\_spacing, r\"$a\\_{}$\".format(m), fontsize\\=12\\*fig\\_scale,\n                                verticalalignment\\='center', horizontalalignment\\='left', zorder\\=5, c\\='b')\n                else:\n                    ax.text(nx, ny, r'$h\\_{}$'.format(m), \\*\\*txtargs)\n    \\# Edges\n    for n, (layer\\_size\\_a, layer\\_size\\_b) in enumerate(zip(layer\\_sizes\\[:\\-1\\], layer\\_sizes\\[1:\\])):\n        layer\\_top\\_a \\= v\\_spacing\\*(layer\\_size\\_a \\- 1)/2. + (top + bottom)/2.\n        layer\\_top\\_b \\= v\\_spacing\\*(layer\\_size\\_b \\- 1)/2. + (top + bottom)/2.\n        for m in range(layer\\_size\\_a):\n            for o in range(layer\\_size\\_b):\n                if not (draw\\_bias and o\\==0 and len(layer\\_sizes)\\>2 and n<layer\\_size\\_b\\-1):\n                    xs \\= \\[n\\*h\\_spacing + left, (n + 1)\\*h\\_spacing + left\\]\n                    ys \\= \\[layer\\_top\\_a \\- m\\*v\\_spacing, layer\\_top\\_b \\- o\\*v\\_spacing\\]\n                    color \\= 'k' if not random\\_weights else plt.cm.bwr(np.random.random())\n                    ax.add\\_artist(plt.Line2D(xs, ys, c\\=color, alpha\\=0.6))\n                    if not (draw\\_bias and m\\==0):\n                        w\\_count += 1\n                    if labels and not random\\_weights:\n                        wl \\= r'$w\\_{{{},{}}}$'.format(m,o) if layer\\_size\\_b\\>1 else r'$w\\_{}$'.format(m)\n                        ax.text(xs\\[0\\]+np.diff(xs)/2, np.mean(ys)\\-np.diff(ys)/9, wl, ha\\='center', va\\='center',\n                                 fontsize\\=12\\*fig\\_scale)\n    \\# Count\n    if weight\\_count:\n        b\\_count \\= b\\_count \\- layer\\_sizes\\[0\\]\n        if draw\\_bias:\n            b\\_count \\= b\\_count \\- (len(layer\\_sizes) \\- 2)\n        ax.text(right\\*1.05, bottom, \"{} weights, {} biases\".format(w\\_count, b\\_count), ha\\='center', va\\='center')\n## Architecture[#](#architecture \"Link to this heading\")\n-   Logistic regression, drawn in a different, neuro-inspired, way\n    -   Linear model: inner product (\\\\(z\\\\)) of input vector \\\\(\\\\mathbf{x}\\\\) and weight vector \\\\(\\\\mathbf{w}\\\\), plus bias \\\\(w\\_0\\\\)\n    -   Logistic (or sigmoid) function maps the output to a probability in \\[0,1\\]\n    -   Uses log loss (cross-entropy) and gradient descent to learn the weights\n\\\\\\[\\\\hat{y}(\\\\mathbf{x}) = \\\\text{sigmoid}(z) = \\\\text{sigmoid}(w\\_0 + \\\\mathbf{w}\\\\mathbf{x}) = \\\\text{sigmoid}(w\\_0 + w\\_1 \\* x\\_1 + w\\_2 \\* x\\_2 +... + w\\_p \\* x\\_p)\\\\\\]\nShow code cell source Hide code cell source\nfig \\= plt.figure(figsize\\=(3\\*fig\\_scale,3\\*fig\\_scale))\nax \\= fig.gca()\ndraw\\_neural\\_net(ax, \\[4, 1\\], activation\\=True, draw\\_bias\\=True, labels\\=True, sigmoid\\=True)\n![../_images/9ff8adaf13b2e98a72d0ae725bfe67cc0bcef299cb6ebdbc7c906740ba66e99b.png](https://ml-course.github.io/master/_images/9ff8adaf13b2e98a72d0ae725bfe67cc0bcef299cb6ebdbc7c906740ba66e99b.png)\n### Basic Architecture[#](#basic-architecture \"Link to this heading\")\n-   Add one (or more) _hidden_ layers \\\\(h\\\\) with \\\\(k\\\\) nodes (or units, cells, neurons)\n    -   Every ‘neuron’ is a tiny function, the network is an arbitrarily complex function\n    -   Weights \\\\(w\\_{i,j}\\\\) between node \\\\(i\\\\) and node \\\\(j\\\\) form a weight matrix \\\\(\\\\mathbf{W}^{(l)}\\\\) per layer \\\\(l\\\\)\n-   Every neuron weights the inputs \\\\(\\\\mathbf{x}\\\\) and passes it through a non-linear activation function\n    -   Activation functions (\\\\(f,g\\\\)) can be different per layer, output \\\\(\\\\mathbf{a}\\\\) is called activation $\\\\(\\\\color{blue}{h(\\\\mathbf{x})} = \\\\color{blue}{\\\\mathbf{a}} = f(\\\\mathbf{z}) = f(\\\\mathbf{W}^{(1)} \\\\color{green}{\\\\mathbf{x}}+\\\\mathbf{w}^{(1)}\\_0) \\\\quad \\\\quad \\\\color{red}{o(\\\\mathbf{x})} = g(\\\\mathbf{W}^{(2)} \\\\color{blue}{\\\\mathbf{a}}+\\\\mathbf{w}^{(2)}\\_0)\\\\)$\nShow code cell source Hide code cell source\nfig, axes \\= plt.subplots(1,2, figsize\\=(10\\*fig\\_scale,5\\*fig\\_scale))\ndraw\\_neural\\_net(axes\\[0\\], \\[2, 3, 1\\],  draw\\_bias\\=True, labels\\=True, weight\\_count\\=True, figsize\\=(4, 4))\ndraw\\_neural\\_net(axes\\[1\\], \\[2, 3, 1\\],  activation\\=True, show\\_activations\\=True, draw\\_bias\\=True,\n                labels\\=True, weight\\_count\\=True,  figsize\\=(4, 4))\n![../_images/f1b83b78fa150b801053748afccbbecc8ae71fa127ad5a57819db6de5f9f6097.png](https://ml-course.github.io/master/_images/f1b83b78fa150b801053748afccbbecc8ae71fa127ad5a57819db6de5f9f6097.png)\n### More layers[#](#more-layers \"Link to this heading\")\n-   Add more layers, and more nodes per layer, to make the model more complex\n    -   For simplicity, we don’t draw the biases (but remember that they are there)\n-   In _dense_ (fully-connected) layers, every previous layer node is connected to all nodes\n-   The output layer can also have multiple nodes (e.g. 1 per class in multi-class classification)\nShow code cell source Hide code cell source\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact\\_manual\n@interact\ndef plot\\_dense\\_net(nr\\_layers\\=(0,6,1), nr\\_nodes\\=(1,12,1)):\n    fig \\= plt.figure(figsize\\=(10\\*fig\\_scale, 5\\*fig\\_scale))\n    ax \\= fig.gca()\n    ax.axis('off')\n    hidden \\= \\[nr\\_nodes\\]\\*nr\\_layers\n    draw\\_neural\\_net(ax, \\[5\\] + hidden + \\[5\\], weight\\_count\\=True, figsize\\=(6, 4))\n    plt.show()\nShow code cell source Hide code cell source\nif not interactive:\n    plot\\_dense\\_net(nr\\_layers\\=6, nr\\_nodes\\=10)\n![../_images/39538afb543359cf8768def5998551bb4396d3977c10c5f370f04db1306100ae.png](https://ml-course.github.io/master/_images/39538afb543359cf8768def5998551bb4396d3977c10c5f370f04db1306100ae.png)\n### Why layers?[#](#why-layers \"Link to this heading\")\n-   Each layer acts as a _filter_ and learns a new _representation_ of the data\n    -   Subsequent layers can learn iterative refinements\n    -   Easier than learning a complex relationship in one go\n-   Example: for image input, each layer yields new (filtered) images\n    -   Can learn multiple mappings at once: weight _tensor_ \\\\(\\\\mathit{W}\\\\) yields activation tensor \\\\(\\\\mathit{A}\\\\)\n    -   From low-level patterns (edges, end-points, …) to combinations thereof\n    -   Each neuron ‘lights up’ if certain patterns occur in the input\n![ml](https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/00_layers2.png)\n### Other architectures[#](#other-architectures \"Link to this heading\")\n-   There exist MANY types of networks for many different tasks\n-   Convolutional nets for image data, Recurrent nets for sequential data,…\n-   Also used to learn representations (embeddings), generate new images, text,…\n![ml](https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/neural_zoo.png)\n## Training Neural Nets[#](#training-neural-nets \"Link to this heading\")\n-   Design the architecture, choose activation functions (e.g. sigmoids)\n-   Choose a way to initialize the weights (e.g. random initialization)\n-   Choose a _loss function_ (e.g. log loss) to measure how well the model fits training data\n-   Choose an _optimizer_ (typically an SGD variant) to update the weights\n![ml](https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/09_overview.png)\n### Mini-batch Stochastic Gradient Descent (recap)[#](#mini-batch-stochastic-gradient-descent-recap \"Link to this heading\")\n1.  Draw a batch of _batch\\_size_ training data \\\\(\\\\mathbf{X}\\\\) and \\\\(\\\\mathbf{y}\\\\)\n2.  _Forward pass_ : pass \\\\(\\\\mathbf{X}\\\\) though the network to yield predictions \\\\(\\\\mathbf{\\\\hat{y}}\\\\)\n3.  Compute the loss \\\\(\\\\mathcal{L}\\\\) (mismatch between \\\\(\\\\mathbf{\\\\hat{y}}\\\\) and \\\\(\\\\mathbf{y}\\\\))\n4.  _Backward pass_ : Compute the gradient of the loss with regard to every weight\n    -   _Backpropagate_ the gradients through all the layers\n5.  Update \\\\(W\\\\): \\\\(W\\_{(i+1)} = W\\_{(i)} - \\\\frac{\\\\partial L(x, W\\_{(i)})}{\\\\partial W} \\* \\\\eta\\\\)\nRepeat until n passes (epochs) are made through the entire training set\nShow code cell source Hide code cell source\n\\# TODO: show the actual weight updates\n@interact\ndef draw\\_updates(iteration\\=(1,100,1)):\n    fig, ax \\= plt.subplots(1, 1, figsize\\=(6\\*fig\\_scale, 4\\*fig\\_scale))\n    np.random.seed(iteration)\n    draw\\_neural\\_net(ax, \\[6,5,5,3\\], labels\\=True, random\\_weights\\=True, show\\_activations\\=True, figsize\\=(6, 4));\n    plt.show()\nShow code cell source Hide code cell source\nif not interactive:\n    draw\\_updates(iteration\\=1)\n![../_images/ac21e2f7e5627c4fc9eccb6b9593a5b7c9b8acd0593e11ddcfd85645069202fc.png](https://ml-course.github.io/master/_images/ac21e2f7e5627c4fc9eccb6b9593a5b7c9b8acd0593e11ddcfd85645069202fc.png)\n### Forward pass[#](#forward-pass \"Link to this heading\")\n-   We can naturally represent the data as _tensors_\n    -   Numerical n-dimensional array (with n axes)\n    -   2D tensor: matrix (samples, features)\n    -   3D tensor: time series (samples, timesteps, features)\n    -   4D tensor: color images (samples, height, width, channels)\n    -   5D tensor: video (samples, frames, height, width, channels)\n![ml](https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/08_timeseries.png) ![ml](https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/08_images.png)\n#### Tensor operations[#](#tensor-operations \"Link to this heading\")\n-   The operations that the network performs on the data can be reduced to a _series of tensor operations_\n    -   These are also much easier to run on GPUs\n-   A dense layer with sigmoid activation, input tensor \\\\(\\\\mathbf{X}\\\\), weight tensor \\\\(\\\\mathbf{W}\\\\), bias \\\\(\\\\mathbf{b}\\\\):\ny \\= sigmoid(np.dot(X, W) + b)\n-   Tensor dot product for 2D inputs (\\\\(a\\\\) samples, \\\\(b\\\\) features, \\\\(c\\\\) hidden nodes)\n![ml](https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/08_dot.png)\n#### Element-wise operations[#](#element-wise-operations \"Link to this heading\")\n-   Activation functions and addition are element-wise operations:\ndef sigmoid(x):\n  return 1/(1 + np.exp(\\-x))\ndef add(x, y):\n  return x + y\n-   Note: if y has a lower dimension than x, it will be _broadcasted_: axes are added to match the dimensionality, and y is repeated along the new axes\n\\>>> np.array(\\[\\[1,2\\],\\[3,4\\]\\]) + np.array(\\[10,20\\])\narray(\\[\\[11, 22\\],\n       \\[13, 24\\]\\])\n### Backward pass (backpropagation)[#](#backward-pass-backpropagation \"Link to this heading\")\n-   For last layer, compute gradient of the loss function \\\\(\\\\mathcal{L}\\\\) w.r.t all weights of layer \\\\(l\\\\)\n\\\\\\[\\\\begin{split}\\\\nabla \\\\mathcal{L} = \\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial W^{(l)}} = \\\\begin{bmatrix} \\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial w\\_{0,0}} & \\\\ldots & \\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial w\\_{0,l}} \\\\\\\\ \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\ \\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial w\\_{k,0}} & \\\\ldots & \\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial w\\_{k,l}} \\\\end{bmatrix}\\\\end{split}\\\\\\]\n-   Sum up the gradients for all \\\\(\\\\mathbf{x}\\_j\\\\) in minibatch: \\\\(\\\\sum\\_{j} \\\\frac{\\\\partial \\\\mathcal{L}(\\\\mathbf{x}\\_j,y\\_j)}{\\\\partial W^{(l)}}\\\\)\n-   Update all weights in a layer at once (with learning rate \\\\(\\\\eta\\\\)): \\\\(W\\_{(i+1)}^{(l)} = W\\_{(i)}^{(l)} - \\\\eta \\\\sum\\_{j} \\\\frac{\\\\partial \\\\mathcal{L}(\\\\mathbf{x}\\_j,y\\_j)}{\\\\partial W\\_{(i)}^{(l)}}\\\\)\n-   Repeat for next layer, iterating backwards (most efficient, avoids redundant calculations)\n![ml](https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/01_gradient_descent.jpg)\n#### Example[#](#example \"Link to this heading\")\n-   Imagine feeding a single data point, output is \\\\(\\\\hat{y} = g(z) = g(w\\_0 + w\\_1 \\* a\\_1 + w\\_2 \\* a\\_2 +... + w\\_p \\* a\\_p)\\\\)\n-   Decrease loss by updating weights:\n    -   Update the weights of last layer to maximize improvement: \\\\(w\\_{i,(new)} = w\\_{i} - \\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial w\\_i} \\* \\\\eta\\\\)\n    -   To compute gradient \\\\(\\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial w\\_i}\\\\) we need the chain rule: \\\\(f(g(x)) = f'(g(x)) \\* g'(x)\\\\) $\\\\(\\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial w\\_i} = \\\\color{red}{\\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial g}} \\\\color{blue}{\\\\frac{\\\\partial \\\\mathcal{g}}{\\\\partial z\\_0}} \\\\color{green}{\\\\frac{\\\\partial \\\\mathcal{z\\_0}}{\\\\partial w\\_i}}\\\\)$\n-   E.g., with \\\\(\\\\mathcal{L} = \\\\frac{1}{2}(y-\\\\hat{y})^2\\\\) and sigmoid \\\\(\\\\sigma\\\\): \\\\(\\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial w\\_i} = \\\\color{red}{(y - \\\\hat{y})} \\* \\\\color{blue}{\\\\sigma'(z\\_0)} \\* \\\\color{green}{a\\_i}\\\\)\nShow code cell source Hide code cell source\nfig \\= plt.figure(figsize\\=(4\\*fig\\_scale, 3.5\\*fig\\_scale))\nax \\= fig.gca()\ndraw\\_neural\\_net(ax, \\[2, 3, 1\\],  activation\\=True, draw\\_bias\\=True, labels\\=True,\n                show\\_activations\\=True)\n![../_images/7f33ddaf0424bbdd24aea0076e722930175bca525f7dcf01dfe22f11d7e9a35d.png](https://ml-course.github.io/master/_images/7f33ddaf0424bbdd24aea0076e722930175bca525f7dcf01dfe22f11d7e9a35d.png)\n#### Backpropagation (2)[#](#backpropagation-2 \"Link to this heading\")\n-   Another way to decrease the loss \\\\(\\\\mathcal{L}\\\\) is to update the activations \\\\(a\\_i\\\\)\n    -   To update \\\\(a\\_i = f(z\\_i)\\\\), we need to update the weights of the previous layer\n    -   We want to nudge \\\\(a\\_i\\\\) in the right direction by updating \\\\(w\\_{i,j}\\\\): $\\\\(\\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial w\\_{i,j}} = \\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial a\\_i} \\\\frac{\\\\partial a\\_i}{\\\\partial z\\_i} \\\\frac{\\\\partial \\\\mathcal{z\\_i}}{\\\\partial w\\_{i,j}} = \\\\left( \\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial g} \\\\frac{\\\\partial \\\\mathcal{g}}{\\\\partial z\\_0} \\\\frac{\\\\partial \\\\mathcal{z\\_0}}{\\\\partial a\\_i} \\\\right) \\\\frac{\\\\partial a\\_i}{\\\\partial z\\_i} \\\\frac{\\\\partial \\\\mathcal{z\\_i}}{\\\\partial w\\_{i,j}}\\\\)$\n    -   We know \\\\(\\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial g}\\\\) and \\\\(\\\\frac{\\\\partial \\\\mathcal{g}}{\\\\partial z\\_0}\\\\) from the previous step, \\\\(\\\\frac{\\\\partial \\\\mathcal{z\\_0}}{\\\\partial a\\_i} = w\\_i\\\\), \\\\(\\\\frac{\\\\partial a\\_i}{\\\\partial z\\_i} = f'\\\\) and \\\\(\\\\frac{\\\\partial \\\\mathcal{z\\_i}}{\\\\partial w\\_{i,j}} = x\\_j\\\\)\nShow code cell source Hide code cell source\nfig \\= plt.figure(figsize\\=(4\\*fig\\_scale, 4\\*fig\\_scale))\nax \\= fig.gca()\ndraw\\_neural\\_net(ax, \\[2, 3, 1\\],  activation\\=True, draw\\_bias\\=True, labels\\=True,\n                show\\_activations\\=True)\n![../_images/6410abf6583967aa6cfe329c78e6b4b283b3561049807da1eb7ab2c0ef011753.png](https://ml-course.github.io/master/_images/6410abf6583967aa6cfe329c78e6b4b283b3561049807da1eb7ab2c0ef011753.png)\n#### Backpropagation (3)[#](#backpropagation-3 \"Link to this heading\")\n-   With multiple output nodes, \\\\(\\\\mathcal{L}\\\\) is the sum of all per-output (per-class) losses\n    -   \\\\(\\\\frac{\\\\partial \\\\mathcal{L}}{\\\\partial a\\_i}\\\\) is sum of the gradients for every output\n-   Per layer, sum up gradients for every point \\\\(\\\\mathbf{x}\\\\) in the batch: \\\\(\\\\sum\\_{j} \\\\frac{\\\\partial \\\\mathcal{L}(\\\\mathbf{x}\\_j,y\\_j)}{\\\\partial W}\\\\)\n-   Update all weights of every layer \\\\(l\\\\)\n    -   \\\\(W\\_{(i+1)}^{(l)} = W\\_{(i)}^{(l)} - \\\\eta \\\\sum\\_{j} \\\\frac{\\\\partial \\\\mathcal{L}(\\\\mathbf{x}\\_j,y\\_j)}{\\\\partial W\\_{(i)}^{(l)}}\\\\)\n-   Repeat with a new batch of data until loss converges\n-   [Nice animation of the entire process](https://youtu.be/Ilg3gGewQ5U?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;t=403)\nShow code cell source Hide code cell source\nfig \\= plt.figure(figsize\\=(8\\*fig\\_scale, 4\\*fig\\_scale))\nax \\= fig.gca()\ndraw\\_neural\\_net(ax, \\[2, 3, 3, 2\\],  activation\\=True, draw\\_bias\\=True, labels\\=True,\n                random\\_weights\\=True, show\\_activations\\=True, figsize\\=(8, 4))\n![../_images/ec362906f18a38f65157554ce88e629c7aa708c8eb8f4667ac2275c8d8b3d703.png](https://ml-course.github.io/master/_images/ec362906f18a38f65157554ce88e629c7aa708c8eb8f4667ac2275c8d8b3d703.png)\n#### Summary[#](#summary \"Link to this heading\")\n-   The network output \\\\(a\\_o\\\\) is defined by the weights \\\\(W^{(o)}\\\\) and biases \\\\(\\\\mathbf{b}^{(o)}\\\\) of the output layer, and\n-   The activations of a hidden layer \\\\(h\\_1\\\\) with activation function \\\\(a\\_{h\\_1}\\\\), weights \\\\(W^{(1)}\\\\) and biases \\\\(\\\\mathbf{b^{(1)}}\\\\):\n\\\\\\[\\\\color{red}{a\\_o(\\\\mathbf{x})} = \\\\color{red}{a\\_o(\\\\mathbf{z\\_0})} = \\\\color{red}{a\\_o(W^{(o)}} \\\\color{blue}{a\\_{h\\_1}(z\\_{h\\_1})} \\\\color{red}{+ \\\\mathbf{b}^{(o)})} = \\\\color{red}{a\\_o(W^{(o)}} \\\\color{blue}{a\\_{h\\_1}(W^{(1)} \\\\color{green}{\\\\mathbf{x}} + \\\\mathbf{b}^{(1)})} \\\\color{red}{+ \\\\mathbf{b}^{(o)})} \\\\\\]\n-   Minimize the loss by SGD. For layer \\\\(l\\\\), compute \\\\(\\\\frac{\\\\partial \\\\mathcal{L}(a\\_o(x))}{\\\\partial W\\_l}\\\\) and \\\\(\\\\frac{\\\\partial \\\\mathcal{L}(a\\_o(x))}{\\\\partial b\\_{l,i}}\\\\) using the chain rule\n-   Decomposes into gradient of layer above, gradient of activation function, gradient of layer input:\n\\\\\\[\\\\frac{\\\\partial \\\\mathcal{L}(a\\_o)}{\\\\partial W^{(1)}} = \\\\color{red}{\\\\frac{\\\\partial \\\\mathcal{L}(a\\_o)}{\\\\partial a\\_{h\\_1}}} \\\\color{blue}{\\\\frac{\\\\partial a\\_{h\\_1}}{\\\\partial z\\_{h\\_1}}} \\\\color{green}{\\\\frac{\\\\partial z\\_{h\\_1}}{\\\\partial W^{(1)}}} = \\\\left( \\\\color{red}{\\\\frac{\\\\partial \\\\mathcal{L}(a\\_o)}{\\\\partial a\\_o}} \\\\color{blue}{\\\\frac{\\\\partial a\\_o}{\\\\partial z\\_o}} \\\\color{green}{\\\\frac{\\\\partial z\\_o}{\\\\partial a\\_{h\\_1}}}\\\\right) \\\\color{blue}{\\\\frac{\\\\partial a\\_{h\\_1}}{\\\\partial z\\_{h\\_1}}} \\\\color{green}{\\\\frac{\\\\partial z\\_{h\\_1}}{\\\\partial W^{(1)}}} \\\\\\]\n![ml](https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/backprop_schema2.png)\n## Activation functions for hidden layers[#](#activation-functions-for-hidden-layers \"Link to this heading\")\n-   Sigmoid: \\\\(f(z) = \\\\frac{1}{1+e^{-z}}\\\\)\n-   Tanh: \\\\(f(z) = \\\\frac{2}{1+e^{-2z}} - 1\\\\)\n    -   Activations around 0 are better for gradient descent convergence\n-   Rectified Linear (ReLU): \\\\(f(z) = max(0,z)\\\\)\n    -   Less smooth, but much faster (note: not differentiable at 0)\n-   Leaky ReLU: \\\\(f(z) = \\\\begin{cases} 0.01z & z<0 \\\\\\\\ z & otherwise \\\\end{cases}\\\\)\nShow code cell source Hide code cell source\ndef activation(X, function\\=\"sigmoid\"):\n    if function \\== \"sigmoid\":\n        return 1.0/(1.0 + np.exp(\\-X))\n    if function \\== \"softmax\":\n        return np.exp(X) / np.sum(np.exp(X), axis\\=0)\n    elif function \\== \"tanh\":\n        return np.tanh(X)\n    elif function \\== \"relu\":\n        return np.maximum(0,X)\n    elif function \\== \"leaky\\_relu\":\n        return np.maximum(0.1\\*X,X)\n    elif function \\== \"none\":\n        return X\ndef activation\\_derivative(X, function\\=\"sigmoid\"):\n    if function \\== \"sigmoid\":\n        sig \\= 1.0/(1.0 + np.exp(\\-X))\n        return sig \\* (1 \\- sig)\n    elif function \\== \"tanh\":\n        return 1 \\- np.tanh(X)\\*\\*2\n    elif function \\== \"relu\":\n        return np.where(X \\> 0, 1, 0)\n    elif function \\== \"leaky\\_relu\":\n        \\# Using 0.1 instead of 0.01 to make it visible in the plot\n        return np.where(X \\> 0, 1, 0.1)\n    elif function \\== \"none\":\n        return X/X\ndef plot\\_activation(function, ax, derivative\\=False):\n    if function\\==\"softmax\":\n        x \\= np.linspace(\\-6,6,9)\n        ax.plot(x,activation(x, function),lw\\=2, c\\='b', linestyle\\='-', marker\\='o')\n    else:\n        x \\= np.linspace(\\-6,6,101)\n        ax.plot(x,activation(x, function),lw\\=2, c\\='b', linestyle\\='-')\n        if derivative:\n            if function \\== \"relu\" or function \\== \"leaky\\_relu\":\n                ax.step(x,activation\\_derivative(x, function),lw\\=2, c\\='r', linestyle\\='-')\n            else:\n                ax.plot(x,activation\\_derivative(x, function),lw\\=2, c\\='r', linestyle\\='-')\n    ax.set\\_xlabel(\"input\", fontsize\\=16\\*fig\\_scale)\n    ax.set\\_ylabel(function, fontsize\\=18\\*fig\\_scale)\n    ax.tick\\_params(axis\\='both', labelsize\\=16\\*fig\\_scale)\n    ax.grid()\nfunctions \\= \\[\"sigmoid\",\"tanh\",\"relu\",\"leaky\\_relu\"\\]\n@interact\ndef plot\\_activations(function\\=functions):\n    fig, ax \\= plt.subplots(figsize\\=(5,1.5))\n    plot\\_activation(function, ax)\n    plt.show()\nShow code cell source Hide code cell source\nif not interactive:\n    fig, axes \\= plt.subplots(1,4, figsize\\=(10,2))\n    for function, ax in zip(functions,axes):\n        plot\\_activation(function, ax)\n    plt.tight\\_layout();\n![../_images/d735e76d44fe3db245923b7a2cb753338bbafc6d1b71fc4c081cbd865f06a348.png](https://ml-course.github.io/master/_images/d735e76d44fe3db245923b7a2cb753338bbafc6d1b71fc4c081cbd865f06a348.png)\n### Effect of activation functions on the gradient[#](#effect-of-activation-functions-on-the-gradient \"Link to this heading\")\n-   During gradient descent, the gradient depends on the activation function \\\\(a\\_{h}\\\\): \\\\(\\\\frac{\\\\partial \\\\mathcal{L}(a\\_o)}{\\\\partial W^{(l)}} = \\\\color{red}{\\\\frac{\\\\partial \\\\mathcal{L}(a\\_o)}{\\\\partial a\\_{h\\_l}}} \\\\color{blue}{\\\\frac{\\\\partial a\\_{h\\_l}}{\\\\partial z\\_{h\\_l}}} \\\\color{green}{\\\\frac{\\\\partial z\\_{h\\_l}}{\\\\partial W^{(l)}}}\\\\)\n-   If derivative of the activation function \\\\(\\\\color{blue}{\\\\frac{\\\\partial a\\_{h\\_l}}{\\\\partial z\\_{h\\_l}}}\\\\) is 0, the weights \\\\(w\\_i\\\\) are not updated\n    -   Moreover, the gradients of previous layers will be reduced (vanishing gradient)\n-   sigmoid, tanh: gradient is very small for large inputs: slow updates\n-   With ReLU, \\\\(\\\\color{blue}{\\\\frac{\\\\partial a\\_{h\\_l}}{\\\\partial z\\_{h\\_l}}} = 1\\\\) if \\\\(z>0\\\\), hence better against vanishing gradients\n    -   Problem: for very negative inputs, the gradient is 0 and may never recover (dying ReLU)\n    -   Leaky ReLU has a small (0.01) gradient there to allow recovery\nShow code cell source Hide code cell source\n@interact\ndef plot\\_activations\\_derivative(function\\=functions):\n    fig, ax \\= plt.subplots(figsize\\=(6,2))\n    plot\\_activation(function, ax, derivative\\=True)\n    plt.legend(\\['original','derivative'\\], loc\\='upper center',\n               bbox\\_to\\_anchor\\=(0.5, 1.25), ncol\\=2)\n    plt.show()\nShow code cell source Hide code cell source\nif not interactive:\n    fig, axes \\= plt.subplots(1,4, figsize\\=(10,2))\n    for function, ax in zip(functions,axes):\n        plot\\_activation(function, ax, derivative\\=True)\n    fig.legend(\\['original','derivative'\\], loc\\='upper center',\n               bbox\\_to\\_anchor\\=(0.5, 1.25), ncol\\=2)\n    plt.tight\\_layout();\n![../_images/c53fc05c1577c4cd4f399c80d37ce2d7458603138743d48cbe4b468452c37bb6.png](https://ml-course.github.io/master/_images/c53fc05c1577c4cd4f399c80d37ce2d7458603138743d48cbe4b468452c37bb6.png)\n### ReLU vs Tanh[#](#relu-vs-tanh \"Link to this heading\")\n-   What is the effect of using non-smooth activation functions?\n    -   ReLU produces piecewise-linear boundaries, but allows deeper networks\n    -   Tanh produces smoother decision boundaries, but is slower\nShow code cell source Hide code cell source\nfrom sklearn.neural\\_network import MLPClassifier\nfrom sklearn.datasets import make\\_moons\nfrom sklearn.model\\_selection import train\\_test\\_split\nfrom mglearn.plot\\_2d\\_separator import plot\\_2d\\_classification\nimport time\n@interact\ndef plot\\_boundary(nr\\_layers\\=(1,4,1)):\n    X, y \\= make\\_moons(n\\_samples\\=100, noise\\=0.25, random\\_state\\=3)\n    X\\_train, X\\_test, y\\_train, y\\_test \\= train\\_test\\_split(X, y, stratify\\=y,\n                                                        random\\_state\\=42)\n    \\# Multi-Layer Perceptron with ReLU\n    mlp \\= MLPClassifier(solver\\='lbfgs', random\\_state\\=0,\n                        hidden\\_layer\\_sizes\\=\\[10\\]\\*nr\\_layers)\n    start \\= time.time()\n    mlp.fit(X\\_train, y\\_train)\n    relu\\_time \\= time.time() \\- start\n    relu\\_acc \\= mlp.score(X\\_test, y\\_test)\n    \\# Multi-Layer Perceptron with tanh\n    mlp\\_tanh \\= MLPClassifier(solver\\='lbfgs', activation\\='tanh',\n                             random\\_state\\=0, hidden\\_layer\\_sizes\\=\\[10\\]\\*nr\\_layers)\n    start \\= time.time()\n    mlp\\_tanh.fit(X\\_train, y\\_train)\n    tanh\\_time \\= time.time() \\- start\n    tanh\\_acc \\= mlp\\_tanh.score(X\\_test, y\\_test)\n    fig, axes \\= plt.subplots(1, 2, figsize\\=(8\\*fig\\_scale, 4\\*fig\\_scale))\n    axes\\[0\\].scatter(X\\_train\\[:, 0\\], X\\_train\\[:, 1\\], c\\=y\\_train, cmap\\='bwr', label\\=\"train\")\n    axes\\[0\\].set\\_title(\"ReLU, acc: {:.2f}, time: {:.2f} sec\".format(relu\\_acc, relu\\_time))\n    plot\\_2d\\_classification(mlp, X\\_train, fill\\=True, cm\\='bwr', alpha\\=.3, ax\\=axes\\[0\\])\n    axes\\[1\\].scatter(X\\_train\\[:, 0\\], X\\_train\\[:, 1\\], c\\=y\\_train, cmap\\='bwr', label\\=\"train\")\n    axes\\[1\\].set\\_title(\"tanh, acc: {:.2f}, time: {:.2f} sec\".format(tanh\\_acc, tanh\\_time))\n    plot\\_2d\\_classification(mlp\\_tanh, X\\_train, fill\\=True, cm\\='bwr', alpha\\=.3, ax\\=axes\\[1\\])\n    plt.show()\nShow code cell source Hide code cell source\nif not interactive:\n    plot\\_boundary(nr\\_layers\\=2)\n![../_images/df30627dd0a20f6214070762c0909a252583a675c91faa617227320b632db8b7.png](https://ml-course.github.io/master/_images/df30627dd0a20f6214070762c0909a252583a675c91faa617227320b632db8b7.png)\n### Activation functions for output layer[#](#activation-functions-for-output-layer \"Link to this heading\")\n-   _sigmoid_ converts output to probability in \\[0,1\\]\n    -   For binary classification\n-   _softmax_ converts all outputs (aka ‘logits’) to probabilities that sum up to 1\n    -   For multi-class classification (\\\\(k\\\\) classes)\n    -   Can cause over-confident models. If so, smooth the labels: \\\\(y\\_{smooth} = (1-\\\\alpha)y + \\\\frac{\\\\alpha}{k}\\\\) $\\\\(\\\\text{softmax}(\\\\mathbf{x},i) = \\\\frac{e^{x\\_i}}{\\\\sum\\_{j=1}^k e^{x\\_j}}\\\\)$\n-   For regression, don’t use any activation function, let the model learn the exact target\nShow code cell source Hide code cell source\noutput\\_functions \\= \\[\"sigmoid\",\"softmax\",\"none\"\\]\n@interact\ndef plot\\_output\\_activation(function\\=output\\_functions):\n    fig, ax \\= plt.subplots(figsize\\=(6,2))\n    plot\\_activation(function, ax)\n    plt.show()\nShow code cell source Hide code cell source\nif not interactive:\n    fig, axes \\= plt.subplots(1,2, figsize\\=(8,2))\n    for function, ax in zip(output\\_functions\\[:2\\],axes):\n        plot\\_activation(function, ax)\n    plt.tight\\_layout();\n![../_images/9a5ec41d73ecc4d3074b0e4fbf24e34509af0a20269aa0c48c8ceef0a39a9d56.png](https://ml-course.github.io/master/_images/9a5ec41d73ecc4d3074b0e4fbf24e34509af0a20269aa0c48c8ceef0a39a9d56.png)\n## Weight initialization[#](#weight-initialization \"Link to this heading\")\n-   Initializing weights to 0 is bad: all gradients in layer will be identical (symmetry)\n-   Too small random weights shrink activations to 0 along the layers (vanishing gradient)\n-   Too large random weights multiply along layers (exploding gradient, zig-zagging)\n-   Ideal: small random weights + variance of input and output gradients remains the same\n    -   Glorot/Xavier initialization (for tanh): randomly sample from \\\\(N(0,\\\\sigma), \\\\sigma = \\\\sqrt{\\\\frac{2}{\\\\text{fan\\_in + fan\\_out}}}\\\\)\n        -   fan\\_in: number of input units, fan\\_out: number of output units\n    -   He initialization (for ReLU): randomly sample from \\\\(N(0,\\\\sigma), \\\\sigma = \\\\sqrt{\\\\frac{2}{\\\\text{fan\\_in}}}\\\\)\n    -   Uniform sampling (instead of \\\\(N(0,\\\\sigma)\\\\)) for deeper networks (w.r.t. vanishing gradients)\nShow code cell source Hide code cell source\nfig, ax \\= plt.subplots(1,1, figsize\\=(6\\*fig\\_scale, 3\\*fig\\_scale))\ndraw\\_neural\\_net(ax, \\[3, 5, 5, 5, 5, 5, 3\\], random\\_weights\\=True, figsize\\=(6, 3))\n![../_images/4d186b04063cb77d8f146bad433f162d0054a9ebe7dddae82adc50e3ac19d0b9.png](https://ml-course.github.io/master/_images/4d186b04063cb77d8f146bad433f162d0054a9ebe7dddae82adc50e3ac19d0b9.png)\n### Weight initialization: transfer learning[#](#weight-initialization-transfer-learning \"Link to this heading\")\n-   Instead of starting from scratch, start from weights previously learned from similar tasks\n    -   This is, to a big extent, how humans learn so fast\n-   Transfer learning: learn weights on task T, transfer them to new network\n    -   Weights can be frozen, or finetuned to the new data\n-   Only works if the previous task is ‘similar’ enough\n    -   Generally, weights learned on very diverse data (e.g. ImageNet) transfer better\n    -   Meta-learning: learn a good initialization across many related tasks\n![ml](https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/transfer_learning.png)\nimport tensorflow as tf\n#import tensorflow\\_addons as tfa\n\\# Toy surface\ndef f(x, y):\n    return (1.5 \\- x + x\\*y)\\*\\*2 + (2.25 \\- x + x\\*y\\*\\*2)\\*\\*2 + (2.625 \\- x + x\\*y\\*\\*3)\\*\\*2\n\\# Tensorflow optimizers\nclass CyclicalLearningRate(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def \\_\\_init\\_\\_(self, initial\\_learning\\_rate, maximal\\_learning\\_rate, step\\_size):\n        self.initial\\_learning\\_rate \\= initial\\_learning\\_rate\n        self.maximal\\_learning\\_rate \\= maximal\\_learning\\_rate\n        self.step\\_size \\= step\\_size\n    def \\_\\_call\\_\\_(self, step):\n        cycle \\= tf.floor(1 + step / (2 \\* self.step\\_size))\n        x \\= tf.abs(step / self.step\\_size \\- 2 \\* cycle + 1)\n        return self.initial\\_learning\\_rate + (self.maximal\\_learning\\_rate \\- self.initial\\_learning\\_rate) \\* tf.maximum(0.0, (1 \\- x))\nclr\\_schedule \\= CyclicalLearningRate(initial\\_learning\\_rate\\=1e-4,\n                                    maximal\\_learning\\_rate\\=0.1,\n                                    step\\_size\\=100)\nsgd\\_cyclic \\= tf.keras.optimizers.SGD(learning\\_rate\\=clr\\_schedule)\nsgd \\= tf.optimizers.SGD(0.01)\nlr\\_schedule \\= tf.optimizers.schedules.ExponentialDecay(0.02,decay\\_steps\\=100,decay\\_rate\\=0.96)\nsgd\\_decay \\= tf.optimizers.SGD(learning\\_rate\\=lr\\_schedule)\nmomentum \\= tf.optimizers.SGD(0.005, momentum\\=0.9, nesterov\\=False)\nnesterov \\= tf.optimizers.SGD(0.005, momentum\\=0.9, nesterov\\=True)\nadagrad \\= tf.optimizers.Adagrad(0.4)\nrmsprop \\= tf.optimizers.RMSprop(learning\\_rate\\=0.1)\nadam \\= tf.optimizers.Adam(learning\\_rate\\=0.2, beta\\_1\\=0.9, beta\\_2\\=0.999, epsilon\\=1e-8)\noptimizers \\= \\[sgd, sgd\\_decay, momentum, nesterov, adagrad, rmsprop, adam, sgd\\_cyclic\\]\nopt\\_names \\= \\['sgd', 'sgd\\_decay', 'momentum', 'nesterov', 'adagrad', 'rmsprop', 'adam', 'sgd\\_cyclic'\\]\ncmap \\= plt.cm.get\\_cmap('tab10')\ncolors \\= \\[cmap(x/10) for x in range(10)\\]\n\\# Training\nall\\_paths \\= \\[\\]\nfor opt, name in zip(optimizers, opt\\_names):\n    x \\= tf.Variable(0.8)\n    y \\= tf.Variable(1.6)\n    x\\_history \\= \\[\\]\n    y\\_history \\= \\[\\]\n    loss\\_prev \\= 0.0\n    max\\_steps \\= 100\n    for step in range(max\\_steps):\n        with tf.GradientTape() as g:\n            loss \\= f(x, y)\n            x\\_history.append(x.numpy())\n            y\\_history.append(y.numpy())\n            grads \\= g.gradient(loss, \\[x, y\\])\n            opt.apply\\_gradients(zip(grads, \\[x, y\\]))\n    if np.abs(loss\\_prev \\- loss.numpy()) < 1e-6:\n        break\n    loss\\_prev \\= loss.numpy()\n    x\\_history \\= np.array(x\\_history)\n    y\\_history \\= np.array(y\\_history)\n    path \\= np.concatenate((np.expand\\_dims(x\\_history, 1), np.expand\\_dims(y\\_history, 1)), axis\\=1).T\n    all\\_paths.append(path)\n\\# Plotting\nnumber\\_of\\_points \\= 50\nmargin \\= 4.5\nminima \\= np.array(\\[3., .5\\])\nminima\\_ \\= minima.reshape(\\-1, 1)\nx\\_min \\= 0. \\- 2\nx\\_max \\= 0. + 3.5\ny\\_min \\= 0. \\- 3.5\ny\\_max \\= 0. + 2\nx\\_points \\= np.linspace(x\\_min, x\\_max, number\\_of\\_points)\ny\\_points \\= np.linspace(y\\_min, y\\_max, number\\_of\\_points)\nx\\_mesh, y\\_mesh \\= np.meshgrid(x\\_points, y\\_points)\nz \\= np.array(\\[f(xps, yps) for xps, yps in zip(x\\_mesh, y\\_mesh)\\])\ndef plot\\_optimizers(ax, iterations, optimizers):\n    ax.contour(x\\_mesh, y\\_mesh, z, levels\\=np.logspace(\\-0.5, 5, 25), norm\\=LogNorm(), cmap\\=plt.cm.jet, linewidths\\=fig\\_scale, zorder\\=-1)\n    ax.plot(\\*minima, 'r\\*', markersize\\=20\\*fig\\_scale)\n    for name, path, color in zip(opt\\_names, all\\_paths, colors):\n        if name in optimizers:\n            p \\= path\\[:,:iterations\\]\n            ax.plot(\\[\\], \\[\\], color\\=color, label\\=name, lw\\=3\\*fig\\_scale, linestyle\\='-')\n            ax.quiver(p\\[0,:\\-1\\], p\\[1,:\\-1\\], p\\[0,1:\\]\\-p\\[0,:\\-1\\], p\\[1,1:\\]\\-p\\[1,:\\-1\\], scale\\_units\\='xy', angles\\='xy', scale\\=1, color\\=color, lw\\=4)\n    ax.set\\_xlim((x\\_min, x\\_max))\n    ax.set\\_ylim((y\\_min, y\\_max))\n    ax.legend(loc\\='lower left', prop\\={'size': 15\\*fig\\_scale})\n    ax.set\\_xticks(\\[\\])\n    ax.set\\_yticks(\\[\\])\n    plt.tight\\_layout()\nfrom decimal import \\*\nfrom matplotlib.colors import LogNorm\n\\# Training for momentum\nall\\_lr\\_paths \\= \\[\\]\nlr\\_range \\= \\[0.005 \\* i for i in range(0,10)\\]\nfor lr in lr\\_range:\n    opt \\= tf.optimizers.SGD(lr, nesterov\\=False)\n    x\\_init \\= 0.8\n    x \\= tf.compat.v1.get\\_variable('x', dtype\\=tf.float32, initializer\\=tf.constant(x\\_init))\n    y\\_init \\= 1.6\n    y \\= tf.compat.v1.get\\_variable('y', dtype\\=tf.float32, initializer\\=tf.constant(y\\_init))\n    x\\_history \\= \\[\\]\n    y\\_history \\= \\[\\]\n    z\\_prev \\= 0.0\n    max\\_steps \\= 100\n    for step in range(max\\_steps):\n        with tf.GradientTape() as g:\n            z \\= f(x, y)\n            x\\_history.append(x.numpy())\n            y\\_history.append(y.numpy())\n            dz\\_dx, dz\\_dy \\= g.gradient(z, \\[x, y\\])\n            opt.apply\\_gradients(zip(\\[dz\\_dx, dz\\_dy\\], \\[x, y\\]))\n    if np.abs(z\\_prev \\- z.numpy()) < 1e-6:\n        break\n    z\\_prev \\= z.numpy()\n    x\\_history \\= np.array(x\\_history)\n    y\\_history \\= np.array(y\\_history)\n    path \\= np.concatenate((np.expand\\_dims(x\\_history, 1), np.expand\\_dims(y\\_history, 1)), axis\\=1).T\n    all\\_lr\\_paths.append(path)\n\\# Plotting\nnumber\\_of\\_points \\= 50\nmargin \\= 4.5\nminima \\= np.array(\\[3., .5\\])\nminima\\_ \\= minima.reshape(\\-1, 1)\nx\\_min \\= 0. \\- 2\nx\\_max \\= 0. + 3.5\ny\\_min \\= 0. \\- 3.5\ny\\_max \\= 0. + 2\nx\\_points \\= np.linspace(x\\_min, x\\_max, number\\_of\\_points)\ny\\_points \\= np.linspace(y\\_min, y\\_max, number\\_of\\_points)\nx\\_mesh, y\\_mesh \\= np.meshgrid(x\\_points, y\\_points)\nz \\= np.array(\\[f(xps, yps) for xps, yps in zip(x\\_mesh, y\\_mesh)\\])\ndef plot\\_learning\\_rate\\_optimizers(ax, iterations, lr):\n    ax.contour(x\\_mesh, y\\_mesh, z, levels\\=np.logspace(\\-0.5, 5, 25), norm\\=LogNorm(), cmap\\=plt.cm.jet, linewidths\\=fig\\_scale, zorder\\=-1)\n    ax.plot(\\*minima, 'r\\*', markersize\\=20\\*fig\\_scale)\n    for path, lrate in zip(all\\_lr\\_paths, lr\\_range):\n        if round(lrate,3) \\== lr:\n            p \\= path\\[:,:iterations\\]\n            ax.plot(\\[\\], \\[\\], color\\='b', label\\=\"Learning rate {}\".format(lr), lw\\=3\\*fig\\_scale, linestyle\\='-')\n            ax.quiver(p\\[0,:\\-1\\], p\\[1,:\\-1\\], p\\[0,1:\\]\\-p\\[0,:\\-1\\], p\\[1,1:\\]\\-p\\[1,:\\-1\\], scale\\_units\\='xy', angles\\='xy', scale\\=1, color\\='b', lw\\=4)\n    ax.set\\_xlim((x\\_min, x\\_max))\n    ax.set\\_ylim((y\\_min, y\\_max))\n    ax.legend(loc\\='lower left', prop\\={'size': 15\\*fig\\_scale})\n    ax.set\\_xticks(\\[\\])\n    ax.set\\_yticks(\\[\\])\n    plt.tight\\_layout()\nShow code cell source Hide code cell source\n\\# Toy plot to illustrate nesterov momentum\n\\# TODO: replace with actual gradient computation?\ndef plot\\_nesterov(ax, method\\=\"Nesterov momentum\"):\n    ax.contour(x\\_mesh, y\\_mesh, z, levels\\=np.logspace(\\-0.5, 5, 25), norm\\=LogNorm(), cmap\\=plt.cm.jet, linewidths\\=fig\\_scale, zorder\\=-1)\n    ax.plot(\\*minima, 'r\\*', markersize\\=20\\*fig\\_scale)\n    \\# toy example\n    ax.quiver(\\-0.8,\\-1.13,1,1.33, scale\\_units\\='xy', angles\\='xy', scale\\=1, color\\='k', alpha\\=0.5, lw\\=3, label\\=\"previous update\")\n    \\# 0.9 \\* previous update\n    ax.quiver(0.2,0.2,0.9,1.2, scale\\_units\\='xy', angles\\='xy', scale\\=1, color\\='g', lw\\=3, label\\=\"momentum step\")\n    if method \\== \"Momentum\":\n        ax.quiver(0.2,0.2,0.5,0, scale\\_units\\='xy', angles\\='xy', scale\\=1, color\\='r', lw\\=3, label\\=\"gradient step\")\n        ax.quiver(0.2,0.2,0.9\\*0.9+0.5,1.2, scale\\_units\\='xy', angles\\='xy', scale\\=1, color\\='b', lw\\=3, label\\=\"actual step\")\n    if method \\== \"Nesterov momentum\":\n        ax.quiver(1.1,1.4,\\-0.2,\\-1, scale\\_units\\='xy', angles\\='xy', scale\\=1, color\\='r', lw\\=3, label\\=\"'lookahead' gradient step\")\n        ax.quiver(0.2,0.2,0.7,0.2, scale\\_units\\='xy', angles\\='xy', scale\\=1, color\\='b', lw\\=3, label\\=\"actual step\")\n    ax.set\\_title(method)\n    ax.set\\_xlim((x\\_min, x\\_max))\n    ax.set\\_ylim((\\-2.5, y\\_max))\n    ax.legend(loc\\='lower right', prop\\={'size': 9\\*fig\\_scale})\n    ax.set\\_xticks(\\[\\])\n    ax.set\\_yticks(\\[\\])\n    plt.tight\\_layout()\n## Optimizers[#](#optimizers \"Link to this heading\")\n### SGD with learning rate schedules[#](#sgd-with-learning-rate-schedules \"Link to this heading\")\n-   Using a constant learning \\\\(\\\\eta\\\\) rate for weight updates \\\\(\\\\mathbf{w}\\_{(s+1)} = \\\\mathbf{w}\\_s-\\\\eta\\\\nabla \\\\mathcal{L}(\\\\mathbf{w}\\_s)\\\\) is not ideal\n    -   You would need to ‘magically’ know the right value\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact\\_manual\nfrom IPython.display import clear\\_output\n@interact\ndef plot\\_lr(iterations\\=(1,100,1), learning\\_rate\\=(0.005,0.04,0.005)):\n    fig, ax \\= plt.subplots(figsize\\=(6\\*fig\\_scale,4\\*fig\\_scale))\n    plot\\_learning\\_rate\\_optimizers(ax,iterations,round(learning\\_rate,3))\n    plt.show()\nif not interactive:\n    plot\\_lr(iterations\\=50, learning\\_rate\\=0.02)\n![../_images/c32cd10a9f3fe5176b6f2b088b0a70be0434fd0f7f847fa254de09bde1eca29e.png](https://ml-course.github.io/master/_images/c32cd10a9f3fe5176b6f2b088b0a70be0434fd0f7f847fa254de09bde1eca29e.png)\n### SGD with learning rate schedules[#](#id1 \"Link to this heading\")\n-   Learning rate decay/annealing with decay rate \\\\(k\\\\)\n    -   E.g. exponential (\\\\(\\\\eta\\_{s+1} = \\\\eta\\_{0} e^{-ks}\\\\)), inverse-time (\\\\(\\\\eta\\_{s+1} = \\\\frac{\\\\eta\\_{0}}{1+ks}\\\\)),…\n-   Cyclical learning rates\n    -   Change from small to large: hopefully in ‘good’ region long enough before diverging\n    -   Warm restarts: aggressive decay + reset to initial learning rate\nShow code cell source Hide code cell source\n@interact\ndef compare\\_optimizers(iterations\\=(1,100,1), optimizer1\\=opt\\_names, optimizer2\\=opt\\_names):\n    fig, ax \\= plt.subplots(figsize\\=(6\\*fig\\_scale,4\\*fig\\_scale))\n    plot\\_optimizers(ax,iterations,\\[optimizer1,optimizer2\\])\n    plt.show()\nShow code cell source Hide code cell source\nif not interactive:\n    fig, axes \\= plt.subplots(1,2, figsize\\=(10\\*fig\\_scale,4\\*fig\\_scale))\n    optimizers \\= \\['sgd\\_decay', 'sgd\\_cyclic'\\]\n    for function, ax in zip(optimizers,axes):\n        plot\\_optimizers(ax,100,function)\n    plt.tight\\_layout();\n![../_images/04e6b15a8e1fc87478207003ef1099be531010ad770b764c1accb89d724a969d.png](https://ml-course.github.io/master/_images/04e6b15a8e1fc87478207003ef1099be531010ad770b764c1accb89d724a969d.png)\n### Momentum[#](#momentum \"Link to this heading\")\n-   Imagine a ball rolling downhill: accumulates momentum, doesn’t exactly follow steepest descent\n    -   Reduces oscillation, follows larger (consistent) gradient of the loss surface\n-   Adds a velocity vector \\\\(\\\\mathbf{v}\\\\) with momentum \\\\(\\\\gamma\\\\) (e.g. 0.9, or increase from \\\\(\\\\gamma=0.5\\\\) to \\\\(\\\\gamma=0.99\\\\)) $\\\\(\\\\mathbf{w}\\_{(s+1)} = \\\\mathbf{w}\\_{(s)} + \\\\mathbf{v}\\_{(s)} \\\\qquad \\\\text{with} \\\\qquad \\\\color{blue}{\\\\mathbf{v}\\_{(s)}} = \\\\color{green}{\\\\gamma \\\\mathbf{v}\\_{(s-1)}} - \\\\color{red}{\\\\eta \\\\nabla \\\\mathcal{L}(\\\\mathbf{w}\\_{(s)})}\\\\)$\n-   Nesterov momentum: Look where momentum step would bring you, compute gradient there\n    -   Responds faster (and reduces momentum) when the gradient changes $\\\\(\\\\color{blue}{\\\\mathbf{v}\\_{(s)}} = \\\\color{green}{\\\\gamma \\\\mathbf{v}\\_{(s-1)}} - \\\\color{red}{\\\\eta \\\\nabla \\\\mathcal{L}(\\\\mathbf{w}\\_{(s)} + \\\\gamma \\\\mathbf{v}\\_{(s-1)})}\\\\)$\nShow code cell source Hide code cell source\nfig, axes \\= plt.subplots(1,2, figsize\\=(10\\*fig\\_scale,4\\*fig\\_scale))\nplot\\_nesterov(axes\\[0\\],method\\=\"Momentum\")\nplot\\_nesterov(axes\\[1\\],method\\=\"Nesterov momentum\")\n![../_images/e7432adc6200e9a2c7e89707e33266a5367d6387d037c223a613867899f0d646.png](https://ml-course.github.io/master/_images/e7432adc6200e9a2c7e89707e33266a5367d6387d037c223a613867899f0d646.png)\n#### Momentum in practice[#](#momentum-in-practice \"Link to this heading\")\nShow code cell source Hide code cell source\n@interact\ndef compare\\_optimizers(iterations\\=(1,100,1), optimizer1\\=opt\\_names, optimizer2\\=opt\\_names):\n    fig, ax \\= plt.subplots(figsize\\=(6\\*fig\\_scale,4\\*fig\\_scale))\n    plot\\_optimizers(ax,iterations,\\[optimizer1,optimizer2\\])\n    plt.show()\nShow code cell source Hide code cell source\nif not interactive:\n    fig, axes \\= plt.subplots(1,2, figsize\\=(10\\*fig\\_scale,3.5\\*fig\\_scale))\n    optimizers \\= \\[\\['sgd','momentum'\\], \\['momentum','nesterov'\\]\\]\n    for function, ax in zip(optimizers,axes):\n        plot\\_optimizers(ax,100,function)\n    plt.tight\\_layout();\n![../_images/248b8f48cea90e2e2cc5e461e732ab2d360974b986b850cceac323d3689bfa53.png](https://ml-course.github.io/master/_images/248b8f48cea90e2e2cc5e461e732ab2d360974b986b850cceac323d3689bfa53.png)\n### Adaptive gradients[#](#adaptive-gradients \"Link to this heading\")\n-   ‘Correct’ the learning rate for each \\\\(w\\_i\\\\) based on specific local conditions (layer depth, fan-in,…)\n-   Adagrad: scale \\\\(\\\\eta\\\\) according to squared sum of previous gradients \\\\(G\\_{i,(s)} = \\\\sum\\_{t=1}^s \\\\nabla \\\\mathcal{L}(w\\_{i,(t)})^2\\\\)\n    -   Update rule for \\\\(w\\_i\\\\). Usually \\\\(\\\\epsilon=10^{-7}\\\\) (avoids division by 0), \\\\(\\\\eta=0.001\\\\). $\\\\(w\\_{i,(s+1)} = w\\_{i,(s)} - \\\\frac{\\\\eta}{\\\\sqrt{G\\_{i,(s)}+\\\\epsilon}} \\\\nabla \\\\mathcal{L}(w\\_{i,(s)})\\\\)$\n-   RMSProp: use _moving average_ of squared gradients \\\\(m\\_{i,(s)} = \\\\gamma m\\_{i,(s-1)} + (1-\\\\gamma) \\\\nabla \\\\mathcal{L}(w\\_{i,(s)})^2\\\\)\n    -   Avoids that gradients dwindle to 0 as \\\\(G\\_{i,(s)}\\\\) grows. Usually \\\\(\\\\gamma=0.9, \\\\eta=0.001\\\\) $\\\\(w\\_{i,(s+1)} = w\\_{i,(s)}- \\\\frac{\\\\eta}{\\\\sqrt{m\\_{i,(s)}+\\\\epsilon}} \\\\nabla \\\\mathcal{L}(w\\_{i,(s)})\\\\)$\nShow code cell source Hide code cell source\nif not interactive:\n    fig, axes \\= plt.subplots(1,2, figsize\\=(10\\*fig\\_scale,2.6\\*fig\\_scale))\n    optimizers \\= \\[\\['sgd','adagrad', 'rmsprop'\\], \\['rmsprop','rmsprop\\_mom'\\]\\]\n    for function, ax in zip(optimizers,axes):\n        plot\\_optimizers(ax,100,function)\n    plt.tight\\_layout();\n![../_images/c53cc403ce501754c9ae246462d4b9c390488d39974c6f6ff3ab3a16cc9cc48b.png](https://ml-course.github.io/master/_images/c53cc403ce501754c9ae246462d4b9c390488d39974c6f6ff3ab3a16cc9cc48b.png)\nShow code cell source Hide code cell source\n@interact\ndef compare\\_optimizers(iterations\\=(1,100,1), optimizer1\\=opt\\_names, optimizer2\\=opt\\_names):\n    fig, ax \\= plt.subplots(figsize\\=(6\\*fig\\_scale,4\\*fig\\_scale))\n    plot\\_optimizers(ax,iterations,\\[optimizer1,optimizer2\\])\n    plt.show()\n### Adam (Adaptive moment estimation)[#](#adam-adaptive-moment-estimation \"Link to this heading\")\n-   Adam: RMSProp + momentum. Adds moving average for gradients as well (\\\\(\\\\gamma\\_2\\\\) = momentum):\n    -   Adds a bias correction to avoid small initial gradients: \\\\(\\\\hat{m}\\_{i,(s)} = \\\\frac{m\\_{i,(s)}}{1-\\\\gamma}\\\\) and \\\\(\\\\hat{g}\\_{i,(s)} = \\\\frac{g\\_{i,(s)}}{1-\\\\gamma\\_2}\\\\) $\\\\(g\\_{i,(s)} = \\\\gamma\\_2 g\\_{i,(s-1)} + (1-\\\\gamma\\_2) \\\\nabla \\\\mathcal{L}(w\\_{i,(s)})\\\\)\\\\( \\\\)\\\\(w\\_{i,(s+1)} = w\\_{i,(s)}- \\\\frac{\\\\eta}{\\\\sqrt{\\\\hat{m}\\_{i,(s)}+\\\\epsilon}} \\\\hat{g}\\_{i,(s)}\\\\)$\n-   Adamax: Idem, but use max() instead of moving average: \\\\(u\\_{i,(s)} = max(\\\\gamma u\\_{i,(s-1)}, |\\\\mathcal{L}(w\\_{i,(s)})|)\\\\) $\\\\(w\\_{i,(s+1)} = w\\_{i,(s)}- \\\\frac{\\\\eta}{u\\_{i,(s)}} \\\\hat{g}\\_{i,(s)}\\\\)$\nShow code cell source Hide code cell source\nif not interactive:\n    \\# fig, axes = plt.subplots(1,2, figsize=(10\\*fig\\_scale,2.6\\*fig\\_scale))\n    \\# optimizers = \\[\\['sgd','adam'\\], \\['adam','adamax'\\]\\]\n    \\# for function, ax in zip(optimizers,axes):\n    \\#     plot\\_optimizers(ax,100,function)\n    \\# plt.tight\\_layout();\n    fig, axes \\= plt.subplots(1,1, figsize\\=(5\\*fig\\_scale,2.6\\*fig\\_scale))\n    optimizers \\= \\[\\['sgd','adam'\\]\\]\n    plot\\_optimizers(axes,100,\\['sgd','adam'\\])\n    plt.tight\\_layout();\n![../_images/5be02b6a7ead865f70a22ee9967e547df87d9511537ae9e0fff1687b1ebda853.png](https://ml-course.github.io/master/_images/5be02b6a7ead865f70a22ee9967e547df87d9511537ae9e0fff1687b1ebda853.png)\nShow code cell source Hide code cell source\n@interact\ndef compare\\_optimizers(iterations\\=(1,100,1), optimizer1\\=opt\\_names, optimizer2\\=opt\\_names):\n    fig, ax \\= plt.subplots(figsize\\=(6\\*fig\\_scale,4\\*fig\\_scale))\n    plot\\_optimizers(ax,iterations,\\[optimizer1,optimizer2\\])\n    plt.show()\n### SGD Optimizer Zoo[#](#sgd-optimizer-zoo \"Link to this heading\")\n-   RMSProp often works well, but do try alternatives. For even more optimizers, [see here](https://ruder.io/optimizing-gradient-descent).\nShow code cell source Hide code cell source\nif not interactive:\n    fig, ax \\= plt.subplots(1,1, figsize\\=(10\\*fig\\_scale,5.5\\*fig\\_scale))\n    plot\\_optimizers(ax,100,opt\\_names)\n![../_images/be3aff8b685ccc51bd7f99bf7600e0adfdea55e0a2aaa536022d8c07300bd69f.png](https://ml-course.github.io/master/_images/be3aff8b685ccc51bd7f99bf7600e0adfdea55e0a2aaa536022d8c07300bd69f.png)\nShow code cell source Hide code cell source\n@interact\ndef compare\\_optimizers(iterations\\=(1,100,1)):\n    fig, ax \\= plt.subplots(figsize\\=(10\\*fig\\_scale,6\\*fig\\_scale))\n    plot\\_optimizers(ax,iterations,opt\\_names)\n    plt.show()\nShow code cell source Hide code cell source\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom numpy.random import seed\nfrom tensorflow.random import set\\_seed\nimport random\nimport os\n#Trying to set all seeds\nos.environ\\['PYTHONHASHSEED'\\]\\=str(0)\nrandom.seed(0)\nseed(0)\nset\\_seed(0)\nseed\\_value\\= 0\n## Neural networks in practice[#](#neural-networks-in-practice \"Link to this heading\")\n-   There are many practical courses on training neural nets.\n    -   E.g.: [https://pytorch.org/tutorials/](https://pytorch.org/tutorials/), [fast.ai course](https://course.fast.ai/)\n-   We’ll use PyTorch in these examples and the labs.\n-   Focus on key design decisions, evaluation, and regularization\n-   Running example: Fashion-MNIST\n    -   28x28 pixel images of 10 classes of fashion items\nShow code cell source Hide code cell source\n\\# Download FMINST data\nmnist \\= oml.datasets.get\\_dataset(40996)\nX, y, \\_, \\_ \\= mnist.get\\_data(target\\=mnist.default\\_target\\_attribute, dataset\\_format\\='array');\nX \\= X.reshape(70000, 28, 28)\nfmnist\\_classes \\= {0:\"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", 5: \"Sandal\",\n                  6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}\n\\# Take some random examples\nfrom random import randint\nfig, axes \\= plt.subplots(1, 5,  figsize\\=(10, 5))\nfor i in range(5):\n    n \\= randint(0,70000)\n    axes\\[i\\].imshow(X\\[n\\], cmap\\=plt.cm.gray\\_r)\n    axes\\[i\\].set\\_xticks(\\[\\])\n    axes\\[i\\].set\\_yticks(\\[\\])\n    axes\\[i\\].set\\_xlabel(\"{}\".format(fmnist\\_classes\\[y\\[n\\]\\]))\nplt.show();\n![../_images/a8c28ac8664d72b67537a8dc07cbf7b62cd950aa1d8c815d77270ba37016a526.png](https://ml-course.github.io/master/_images/a8c28ac8664d72b67537a8dc07cbf7b62cd950aa1d8c815d77270ba37016a526.png)\n### Preparing the data[#](#preparing-the-data \"Link to this heading\")\n-   We’ll use feed-forward networks first, so we flatten the input data\n-   Create train-test splits to evaluate the model later\n-   Convert the data (numpy arrays) to PyTorch tensors\n\\# Flatten images, create train-test split\nX\\_flat \\= X.reshape(70000, 28 \\* 28)\nX\\_train, X\\_test, y\\_train, y\\_test \\= train\\_test\\_split(X\\_flat, y, stratify\\=y)\n\\# Convert numpy arrays to PyTorch tensors with correct types\nX\\_train\\_tensor \\= torch.tensor(X\\_train, dtype\\=torch.float32)\ny\\_train\\_tensor \\= torch.tensor(y\\_train, dtype\\=torch.long)\nX\\_test\\_tensor \\= torch.tensor(X\\_test, dtype\\=torch.float32)\ny\\_test\\_tensor \\= torch.tensor(y\\_test, dtype\\=torch.long)\n-   Create data loaders to return data in batches\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\\# Create PyTorch datasets\ntrain\\_dataset \\= TensorDataset(X\\_train\\_tensor, y\\_train\\_tensor)\ntest\\_dataset \\= TensorDataset(X\\_test\\_tensor, y\\_test\\_tensor)\n\\# Create DataLoaders\nbatch\\_size \\= 64\ntrain\\_loader \\= DataLoader(train\\_dataset, batch\\_size\\=batch\\_size, shuffle\\=True)\ntest\\_loader \\= DataLoader(test\\_dataset, batch\\_size\\=batch\\_size, shuffle\\=False)\nimport torch\nfrom sklearn.model\\_selection import train\\_test\\_split\nfrom torch.utils.data import DataLoader, TensorDataset\n\\# Flatten images, create train-test split\nX\\_flat \\= X.reshape(70000, 28 \\* 28)\nX\\_train, X\\_test, y\\_train, y\\_test \\= train\\_test\\_split(X\\_flat, y, stratify\\=y)\n\\# Convert numpy arrays to PyTorch tensors with correct types\nX\\_train\\_tensor \\= torch.tensor(X\\_train, dtype\\=torch.float32)\ny\\_train\\_tensor \\= torch.tensor(y\\_train, dtype\\=torch.long)\nX\\_test\\_tensor \\= torch.tensor(X\\_test, dtype\\=torch.float32)\ny\\_test\\_tensor \\= torch.tensor(y\\_test, dtype\\=torch.long)\n\\# Create PyTorch datasets\ntrain\\_dataset \\= TensorDataset(X\\_train\\_tensor, y\\_train\\_tensor)\ntest\\_dataset \\= TensorDataset(X\\_test\\_tensor, y\\_test\\_tensor)\n\\# Create DataLoaders\nbatch\\_size \\= 32\ntrain\\_loader \\= DataLoader(train\\_dataset, batch\\_size\\=batch\\_size, num\\_workers\\=4, shuffle\\=True, pin\\_memory\\=False)\ntest\\_loader \\= DataLoader(test\\_dataset, batch\\_size\\=batch\\_size, num\\_workers\\=4, shuffle\\=False, pin\\_memory\\=False)\n### Building the network[#](#building-the-network \"Link to this heading\")\n-   PyTorch has a Sequential and Functional API. We’ll use the Sequential API first.\n-   Input layer: a flat vector of 28\\*28 = 784 nodes\n    -   We’ll see how to properly deal with images later\n-   Two dense (Linear) hidden layers: 512 nodes each, ReLU activation\n-   Output layer: 10 nodes (for 10 classes)\n    -   SoftMax not needed, it will be done in the loss function\nimport torch.nn as nn\nmodel \\= nn.Sequential(\n    nn.Linear(28 \\* 28, 512), \\# Layer 1: 28\\*28 inputs to 512 output nodes\n    nn.ReLU(),\n    nn.Linear(512, 512), \\# Layer 2: 512 inputs to 512 output nodes\n    nn.ReLU(),\n    nn.Linear(512, 10), \\# Layer 3: 512 inputs to output nodes\n)\nIn the Functional API, the same network looks like this\nimport torch.nn.functional as F\nclass NeuralNetwork(nn.Module): \\# Class that defines your model\n    def \\_\\_init\\_\\_(self):\n        super(NeuralNetwork, self).\\_\\_init\\_\\_() \\# Components defined in \\_\\_init\\_\\_\n        self.fc1 \\= nn.Linear(28 \\* 28, 512)    \\# Fully connected layers\n        self.fc2 \\= nn.Linear(512, 512)\n        self.fc3 \\= nn.Linear(512, 10)\n    def forward(self, x):        \\# Forward pass and structure of the network\n        x \\= F.relu(self.fc1(x))  \\# Layer 1: Input to FC1, then through ReLU\n        x \\= F.relu(self.fc2(x))  \\# Layer 2: Then though FC2, then ReLU\n        x \\= self.fc3(x)          \\# Layer 3: Then though FC3, then SoftMax\n        return x                 \\# Return output\nmodel \\= NeuralNetwork()\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass NeuralNetwork(nn.Module): \\# Class that defines your model\n    def \\_\\_init\\_\\_(self):\n        super(NeuralNetwork, self).\\_\\_init\\_\\_() \\# Main components often defined in \\_\\_init\\_\\_\n        self.fc1 \\= nn.Linear(28 \\* 28, 512)    \\# Fully connected layers\n        self.fc2 \\= nn.Linear(512, 512)\n        self.fc3 \\= nn.Linear(512, 10)\n    def forward(self, x):                 \\# Forward pass and structure of the network\n        x \\= F.relu(self.fc1(x))           \\# Layer 1: Input to FC1, then through ReLU\n        x \\= F.relu(self.fc2(x))           \\# Layer 2: Then though FC2, then ReLU\n        x \\= self.fc3(x)                   \\# Layer 3: Then though FC3, then SoftMax\n        return x                          \\# Return output\nmodel \\= NeuralNetwork()\n### Choosing loss, optimizer, metrics[#](#choosing-loss-optimizer-metrics \"Link to this heading\")\n-   **Loss function**: Cross-entropy (log loss) for multi-class classification\n-   **Optimizer**: Any of the optimizers we discussed before. RMSprop/Adam usually work well.\n-   **Metrics**: To monitor performance during training and testing, e.g. accuracy\nimport torch.optim as optim\nimport torchmetrics\n\\# Loss function with label smoothing. Also applies softmax internally\ncriterion \\= nn.CrossEntropyLoss(label\\_smoothing\\=0.01)\n\\# Optimizer. Note that we pass the model parameters at creation time.\noptimizer \\= optim.RMSprop(model.parameters(), lr\\=0.001, momentum\\=0.0)\n\\# Accuracy metric\naccuracy\\_metric \\= torchmetrics.Accuracy(task\\=\"multiclass\", num\\_classes\\=10)\nimport torch.optim as optim\nimport torchmetrics\ncross\\_entropy \\= nn.CrossEntropyLoss(label\\_smoothing\\=0.01)\noptimizer \\= optim.RMSprop(model.parameters(), lr\\=0.001, momentum\\=0.0)\naccuracy\\_metric \\= torchmetrics.Accuracy(task\\=\"multiclass\", num\\_classes\\=10)\n### Training on GPU[#](#training-on-gpu \"Link to this heading\")\n-   The `device` is where the training is done. It’s “cpu” by default.\n-   The model, tensors, and metric must all be moved to the _same_ device.\nif torch.cuda.is\\_available():         \\# For CUDA based systems\n    device \\= torch.device(\"cuda\")\nif torch.backends.mps.is\\_available(): \\# For MPS (M1-M4 Mac) based systems\n    device \\= torch.device(\"mps\")\nprint(f\"Used device: {device}\")\n\\# Move models and metrics to \\`device\\`\nmodel.to(device)\naccuracy\\_metric \\= accuracy\\_metric.to(device)\n\\# Move batches one at a time (GPUs have limited memory)\nfor X\\_batch, y\\_batch in train\\_loader:\n    X\\_batch, y\\_batch \\= X\\_batch.to(device), y\\_batch.to(device)\nif torch.cuda.is\\_available():         \\# For CUDA based systems\n    device \\= torch.device(\"cuda\")\nif torch.backends.mps.is\\_available(): \\# For MPS (M1-M4 Mac) based systems\n    device \\= torch.device(\"mps\")\nprint(f\"Used device: {device}\")\n\\# Move everything to \\`device\\`\nmodel.to(device)\naccuracy\\_metric \\= accuracy\\_metric.to(device)\nfor X\\_batch, y\\_batch in train\\_loader:\n    X\\_batch, y\\_batch \\= X\\_batch.to(device), y\\_batch.to(device)\n    break\nUsed device: mps\n### Training loop[#](#training-loop \"Link to this heading\")\nIn pure PyTorch, you have to write the training loop yourself (as well as any code to print out progress)\nfor epoch in range(10):\n    for X\\_batch, y\\_batch in train\\_loader:\n        X\\_batch, y\\_batch \\= X\\_batch.to(device), y\\_batch.to(device) \\# to GPU\n        \\# Forward pass + loss calculation\n        outputs \\= model(X\\_batch)\n        loss \\= cross\\_entropy(outputs, y\\_batch)\n        \\# Backward pass\n        optimizer.zero\\_grad() \\# Reset gradients (otherwise they accumulate)\n        loss.backward()       \\# Backprop. Computes all gradients\n        optimizer.step()      \\# Uses gradients to update weigths\nnum\\_epochs \\= 5\nfor epoch in range(5):\n    total\\_loss, correct, total \\= 0, 0, 0\n    for X\\_batch, y\\_batch in train\\_loader:\n        X\\_batch, y\\_batch \\= X\\_batch.to(device), y\\_batch.to(device) \\# to GPU\n        \\# Forward pass + loss calculation\n        outputs \\= model(X\\_batch)\n        loss \\= cross\\_entropy(outputs, y\\_batch)\n        \\# Backward pass: zero gradients, do backprop, do optimizer step\n        optimizer.zero\\_grad()\n        loss.backward()\n        optimizer.step()\n        \\# Compute training metrics\n        total\\_loss += loss.item()\n        correct += accuracy\\_metric(outputs, y\\_batch).item() \\* y\\_batch.size(0)\n        total += y\\_batch.size(0)\n    \\# Compute epoch metrics\n    avg\\_loss \\= total\\_loss / len(train\\_loader)\n    avg\\_acc \\= correct / total\n    print(f\"Epoch \\[{epoch+1}/{num\\_epochs}\\], Loss: {avg\\_loss:.4f}, Accuracy: {avg\\_acc:.4f}\")\nEpoch \\[1/5\\], Loss: 2.3524, Accuracy: 0.7495\nEpoch \\[2/5\\], Loss: 0.5531, Accuracy: 0.8259\nEpoch \\[3/5\\], Loss: 0.5102, Accuracy: 0.8408\nEpoch \\[4/5\\], Loss: 0.4897, Accuracy: 0.8493\nEpoch \\[5/5\\], Loss: 0.4758, Accuracy: 0.8550\n### `loss.backward()`[#](#loss-backward \"Link to this heading\")\n-   Every time you perform a forward pass, PyTorch dynamically constructs a _computational graph_\n    -   This graph tracks tensors and operations involved in computing gradients (see next slide)\n-   The `loss` returned is a tensor, and every tensor is part of the computational graph\n-   When you call `.backward()` on `loss`, PyTorch traverses this graph in reverse to compute all gradients\n    -   This process is called _automatic differentiation_\n    -   Stores intermediate values so no gradient component is calculated twice\n-   When `backward()` completes, the computational graph is discarded by default to free memory\nComputational graph for our model (Loss in green, weights/biases in blue)\n![ml](https://ml-course.github.io/master/notebooks/images/cgraph.png)\nfrom torchviz import make\\_dot\nfrom IPython.display import Image\ntoy\\_X \\= X\\_train\\_tensor\\[0\\].to(device)\ntoy\\_y \\= y\\_train\\_tensor\\[0\\].to(device)\ntoy\\_out \\= model(toy\\_X)\nloss \\= cross\\_entropy(toy\\_out, toy\\_y)\ngraph \\= make\\_dot(loss, params\\=dict(model.named\\_parameters()))\ngraph.render(\"computational\\_graph\", format\\=\"png\", view\\=True)\nImage(\"computational\\_graph.png\")\n![../_images/4bca4d09b7b1a585b3c6417f53ddaf6f0d9b04e7ab7656415d4c9830a9971881.png](https://ml-course.github.io/master/_images/4bca4d09b7b1a585b3c6417f53ddaf6f0d9b04e7ab7656415d4c9830a9971881.png)\n### In PyTorch Lightning[#](#in-pytorch-lightning \"Link to this heading\")\n-   A high-level framework built on PyTorch that simplifies deep learning model training\n-   Same code, but extend `pl.LightingModule` instead of `nn.Module`\n-   Has a number of predefined functions. For instance:\nclass NeuralNetwork(pl.LightningModule):\n    def \\_\\_init\\_\\_(self):\n        pass \\# Initialize model\n    def forward(self, x):\n        pass \\# Forward pass, return output tensor\n    def configure\\_optimizers(self):\n        pass \\# Configure optimizer (e.g. Adam)\n    def training\\_step(self, batch, batch\\_idx):\n        pass \\# Return loss tensor\n    def validation\\_step(self, batch, batch\\_idx):\n        pass \\# Return loss tensor\n    def test\\_step(self, batch, batch\\_idx):\n        pass \\# Return loss tensor\nOur entire example now becomes:\nimport pytorch\\_lightning as pl\nclass NeuralNetwork(pl.LightningModule):\n    def \\_\\_init\\_\\_(self):\n        super(LitNeuralNetwork, self).\\_\\_init\\_\\_()\n        self.fc1 \\= nn.Linear(28 \\* 28, 512)\n        self.fc2 \\= nn.Linear(512, 512)\n        self.fc3 \\= nn.Linear(512, 10)\n        self.criterion \\= nn.CrossEntropyLoss(label\\_smoothing\\=0.01)\n        self.accuracy \\= torchmetrics.Accuracy(task\\=\"multiclass\", num\\_classes\\=10)\n    def forward(self, x):\n        x \\= F.relu(self.fc1(x))\n        x \\= F.relu(self.fc2(x))\n        return self.fc3(x)\n    def training\\_step(self, batch, batch\\_idx):\n        X\\_batch, y\\_batch \\= batch\n        outputs \\= self(X\\_batch)\n        return self.criterion(outputs, y\\_batch)\n    def configure\\_optimizers(self):\n        return optim.RMSprop(self.parameters(), lr\\=0.001, momentum\\=0.0)\nmodel \\= NeuralNetwork()\nimport pytorch\\_lightning as pl\nclass NeuralNetwork(pl.LightningModule):\n    def \\_\\_init\\_\\_(self):\n        super(NeuralNetwork, self).\\_\\_init\\_\\_()\n        self.fc1 \\= nn.Linear(28 \\* 28, 512)\n        self.fc2 \\= nn.Linear(512, 512)\n        self.fc3 \\= nn.Linear(512, 10)\n        self.criterion \\= nn.CrossEntropyLoss(label\\_smoothing\\=0.01)\n        self.accuracy \\= torchmetrics.Accuracy(task\\=\"multiclass\", num\\_classes\\=10)\n    def forward(self, x):\n        x \\= x.view(x.size(0), \\-1)  \\# Flatten the image\n        x \\= F.relu(self.fc1(x))\n        x \\= F.relu(self.fc2(x))\n        return self.fc3(x)\n    def training\\_step(self, batch, batch\\_idx):\n        X\\_batch, y\\_batch \\= batch\n        outputs \\= self(X\\_batch)                 \\# Logits (raw outputs)\n        loss \\= self.criterion(outputs, y\\_batch) \\# Loss\n        preds \\= torch.argmax(outputs, dim\\=1)    \\# Predictions\n        acc \\= self.accuracy(preds, y\\_batch)\n        self.log(\"train\\_loss\", loss, sync\\_dist\\=True)\n        self.log(\"train\\_acc\", acc, sync\\_dist\\=True)\n        return loss\n    def validation\\_step(self, batch, batch\\_idx):\n        X\\_batch, y\\_batch \\= batch\n        outputs \\= self(X\\_batch)\n        loss \\= self.criterion(outputs, y\\_batch)\n        preds \\= torch.argmax(outputs, dim\\=1)\n        acc \\= self.accuracy(preds, y\\_batch)\n        self.log(\"val\\_loss\", loss, sync\\_dist\\=True, on\\_epoch\\=True)\n        self.log(\"val\\_acc\", acc, sync\\_dist\\=True, on\\_epoch\\=True)\n        return loss\n    def on\\_train\\_epoch\\_end(self):\n        avg\\_loss \\= self.trainer.callback\\_metrics\\[\"train\\_loss\"\\].item()\n        avg\\_acc \\= self.trainer.callback\\_metrics\\[\"train\\_acc\"\\].item()\n        print(f\"Epoch {self.trainer.current\\_epoch+1}: Loss = {avg\\_loss:.4f}, Accuracy = {avg\\_acc:.4f}\")\n    def configure\\_optimizers(self):\n        return optim.RMSprop(self.parameters(), lr\\=0.001, momentum\\=0.0)\nmodel \\= NeuralNetwork()\nWe can also get a nice model summary\n-   Lots of parameters (weights and biases) to learn!\n    -   hidden layer 1 : (28 \\* 28 + 1) \\* 512 = 401920\n    -   hidden layer 2 : (512 + 1) \\* 512 = 262656\n    -   output layer: (512 + 1) \\* 10 = 5130\nModelSummary(pl\\_model, max\\_depth\\=2)\nShow code cell source Hide code cell source\nfrom pytorch\\_lightning.utilities.model\\_summary import ModelSummary\nModelSummary(model, max\\_depth\\=2)\n  | Name      | Type               | Params | Mode\n---------------------------------------------------------\n0 | fc1       | Linear             | 401 K  | train\n1 | fc2       | Linear             | 262 K  | train\n2 | fc3       | Linear             | 5.1 K  | train\n3 | criterion | CrossEntropyLoss   | 0      | train\n4 | accuracy  | MulticlassAccuracy | 0      | train\n---------------------------------------------------------\n669 K     Trainable params\n0         Non-trainable params\n669 K     Total params\n2.679     Total estimated model params size (MB)\n5         Modules in train mode\n0         Modules in eval mode\n### Training[#](#training \"Link to this heading\")\nTo log results while training, we can extend the training methods:\ndef training\\_step(self, batch, batch\\_idx):\n    X\\_batch, y\\_batch \\= batch\n    outputs \\= self(X\\_batch)                 \\# Logits (raw outputs)\n    loss \\= self.criterion(outputs, y\\_batch) \\# Loss\n    preds \\= torch.argmax(outputs, dim\\=1)    \\# Predictions\n    acc \\= self.accuracy(preds, y\\_batch)     \\# Metric\n    self.log(\"train\\_loss\", loss)            \\# self.log is the default\n    self.log(\"train\\_acc\", acc)              \\# TensorBoard logger\n    return loss\ndef on\\_train\\_epoch\\_end(self): \\# Runs at the end of every epoch\n    avg\\_loss \\= self.trainer.callback\\_metrics\\[\"train\\_loss\"\\].item()\n    avg\\_acc \\= self.trainer.callback\\_metrics\\[\"train\\_acc\"\\].item()\n    print(f\"Epoch {self.trainer.current\\_epoch}: Loss = {avg\\_loss:.4f}, Train accuracy = {avg\\_acc:.4f}\")\nWe also need to implement the validation steps if we want validation scores\n-   Identical to `training_step` except for the logging\ndef validation\\_step(self, batch, batch\\_idx):\n    X\\_batch, y\\_batch \\= batch\n    outputs \\= self(X\\_batch)\n    loss \\= self.criterion(outputs, y\\_batch)\n    preds \\= torch.argmax(outputs, dim\\=1)\n    acc \\= self.accuracy(preds, y\\_batch)\n    self.log(\"val\\_loss\", loss, on\\_epoch\\=True)\n    self.log(\"val\\_acc\", acc, on\\_epoch\\=True)\n    return loss\n### Lightning Trainer[#](#lightning-trainer \"Link to this heading\")\nFor training, we can now create a trainer and fit it. This will also automatically move everything to GPU.\ntrainer \\= pl.Trainer(max\\_epochs\\=3, accelerator\\=\"gpu\") \\# Or 'cpu'\ntrainer.fit(model, train\\_loader)\nimport logging \\# Cleaner output\nlogging.getLogger(\"pytorch\\_lightning\").setLevel(logging.ERROR)\naccelerator \\= \"cpu\"\nif torch.backends.mps.is\\_available():\n    accelerator \\= \"mps\"\nif torch.cuda.is\\_available():\n    accelerator \\= \"gpu\"\ntrainer \\= pl.Trainer(max\\_epochs\\=3, accelerator\\=accelerator)\ntrainer.fit(model, train\\_loader)\nEpoch 1: Loss = 0.6928, Accuracy = 0.8000\nEpoch 2: Loss = 0.3986, Accuracy = 0.9000\nEpoch 3: Loss = 0.3572, Accuracy = 0.9000\n#### Choosing training hyperparameters[#](#choosing-training-hyperparameters \"Link to this heading\")\n-   Number of epochs: enough to allow convergence\n    -   Too much: model starts overfitting (or levels off and just wastes time)\n-   Batch size: small batches (e.g. 32, 64,… samples) often preferred\n    -   ‘Noisy’ training data makes overfitting less likely\n        -   Large batches generalize less well (‘generalization gap’)\n    -   Requires less memory (especially in GPUs)\n    -   Large batches do speed up training, may converge in fewer epochs\n-   [Batch size interacts with learning rate](https://openreview.net/pdf?id=B1Yy1BxCZ)\n    -   Instead of shrinking the learning rate you can increase batch size\n\\# Same model, more silent\nclass NeuralNetwork(pl.LightningModule):\n    def \\_\\_init\\_\\_(self):\n        super(NeuralNetwork, self).\\_\\_init\\_\\_()\n        self.fc1 \\= nn.Linear(28 \\* 28, 512)\n        self.fc2 \\= nn.Linear(512, 512)\n        self.fc3 \\= nn.Linear(512, 10)\n        self.criterion \\= nn.CrossEntropyLoss(label\\_smoothing\\=0.01)\n        self.accuracy \\= torchmetrics.Accuracy(task\\=\"multiclass\", num\\_classes\\=10)\n    def forward(self, x):\n        x \\= x.view(x.size(0), \\-1)  \\# Flatten the image\n        x \\= F.relu(self.fc1(x))\n        x \\= F.relu(self.fc2(x))\n        return self.fc3(x)\n    def training\\_step(self, batch, batch\\_idx):\n        X\\_batch, y\\_batch \\= batch\n        outputs \\= self(X\\_batch)                 \\# Logits (raw outputs)\n        loss \\= self.criterion(outputs, y\\_batch) \\# Loss\n        preds \\= torch.argmax(outputs, dim\\=1)    \\# Predictions\n        acc \\= self.accuracy(preds, y\\_batch)\n        self.log(\"train\\_loss\", loss, sync\\_dist\\=True)\n        self.log(\"train\\_acc\", acc, sync\\_dist\\=True)\n        return loss\n    def validation\\_step(self, batch, batch\\_idx):\n        X\\_batch, y\\_batch \\= batch\n        outputs \\= self(X\\_batch)\n        loss \\= self.criterion(outputs, y\\_batch)\n        preds \\= torch.argmax(outputs, dim\\=1)\n        acc \\= self.accuracy(preds, y\\_batch)\n        self.log(\"val\\_loss\", loss, sync\\_dist\\=True, on\\_epoch\\=True)\n        self.log(\"val\\_acc\", acc, sync\\_dist\\=True, on\\_epoch\\=True)\n        return loss\n    def on\\_train\\_epoch\\_end(self):\n        avg\\_loss \\= self.trainer.callback\\_metrics\\[\"train\\_loss\"\\].item()\n        avg\\_acc \\= self.trainer.callback\\_metrics\\[\"train\\_acc\"\\].item()\n    def on\\_validation\\_epoch\\_end(self):\n        avg\\_loss \\= self.trainer.callback\\_metrics\\[\"val\\_loss\"\\].item()\n        avg\\_acc \\= self.trainer.callback\\_metrics\\[\"val\\_acc\"\\].item()\n    def configure\\_optimizers(self):\n        return optim.RMSprop(self.parameters(), lr\\=0.001, momentum\\=0.0)\nmodel \\= NeuralNetwork()\nfrom pytorch\\_lightning.callbacks import Callback\nfrom IPython.display import clear\\_output\n\\# First try. Not very efficient yet.\nclass TrainingBatchPlotCallback(Callback):\n    def \\_\\_init\\_\\_(self, update\\_interval\\=100, smoothing\\=100):\n        \"\"\"\n        Callback to plot training and validation progress.\n        Args:\n            update\\_interval (int): Number of batches between updates.\n            smoothing (int): Window size for moving average.\n        \"\"\"\n        super().\\_\\_init\\_\\_()\n        self.update\\_interval \\= update\\_interval\n        self.smoothing \\= smoothing\n        self.losses \\= \\[\\]\n        self.acc \\= \\[\\]\n        self.val\\_losses \\= \\[\\]\n        self.val\\_acc \\= \\[\\]\n        self.val\\_x\\_positions \\= \\[\\]  \\# Store x positions for validation scores\n        self.max\\_acc \\= 0\n        self.global\\_step \\= 0  \\# Tracks processed training batches\n        self.epoch\\_count \\= 0  \\# Tracks processed epochs\n        self.steps\\_per\\_epoch \\= 0  \\# Updated dynamically to track num\\_batches per epoch\n    def moving\\_average(self, values, window):\n        \"\"\"Computes a simple moving average for smoothing.\"\"\"\n        if len(values) < window:\n            return np.convolve(values, np.ones(len(values)) / len(values), mode\\=\"valid\")\n        return np.convolve(values, np.ones(window) / window, mode\\=\"valid\")\n    def on\\_train\\_batch\\_end(self, trainer, pl\\_module, outputs, batch, batch\\_idx):\n        \"\"\"Updates training loss and accuracy every \\`update\\_interval\\` batches.\"\"\"\n        self.global\\_step += 1\n        logs \\= trainer.callback\\_metrics\n        \\# Store training loss and accuracy\n        if \"train\\_loss\" in logs:\n            self.losses.append(logs\\[\"train\\_loss\"\\].item())\n        if \"train\\_acc\" in logs:\n            self.acc.append(logs\\[\"train\\_acc\"\\].item())\n        \\# Update plot every \\`update\\_interval\\` training batches\n        if self.global\\_step % self.update\\_interval \\== 0:\n            self.plot\\_progress()\n    def on\\_validation\\_epoch\\_end(self, trainer, pl\\_module):\n        \"\"\"Updates validation loss and accuracy at the end of each epoch, scaling x-axis correctly.\"\"\"\n        logs \\= trainer.callback\\_metrics\n        self.epoch\\_count += 1\n        if self.steps\\_per\\_epoch \\== 0:\n            self.steps\\_per\\_epoch \\= self.global\\_step\n        self.val\\_x\\_positions.append(self.global\\_step)\n        \\# Retrieve validation metrics from trainer\n        if \"val\\_loss\" in logs:\n            self.val\\_losses.append(logs\\[\"val\\_loss\"\\].item())\n        if \"val\\_acc\" in logs:\n            val\\_acc \\= logs\\[\"val\\_acc\"\\].item()\n            self.val\\_acc.append(val\\_acc)\n            self.max\\_acc \\= max(self.max\\_acc, val\\_acc)\n    def on\\_train\\_end(self, trainer, pl\\_module):\n        \"\"\"Ensures the final validation curve is plotted after the last epoch.\"\"\"\n        self.plot\\_progress()\n    def plot\\_progress(self):\n        \"\"\"Plots training and validation metrics with moving averages.\"\"\"\n        clear\\_output(wait\\=True)\n        steps \\= np.arange(1, len(self.losses) + 1)\n        plt.figure(figsize\\=(8, 4))\n        plt.ylim(0, 1)  \\# Constrain y-axis between 0 and 1\n        \\# Training curves\n        plt.plot(steps / self.steps\\_per\\_epoch, self.losses, lw\\=0.2, alpha\\=0.3, c\\=\"b\", linestyle\\=\"-\")\n        plt.plot(steps / self.steps\\_per\\_epoch, self.acc, lw\\=0.2, alpha\\=0.3, c\\=\"r\", linestyle\\=\"-\")\n        \\# Moving average (thicker line)\n        if len(self.losses) \\>= self.smoothing:\n            plt.plot(steps\\[self.smoothing \\- 1:\\] / self.steps\\_per\\_epoch, self.moving\\_average(self.losses, self.smoothing),\n                     lw\\=1, c\\=\"b\", linestyle\\=\"-\", label\\=\"Train Loss\")\n        if len(self.acc) \\>= self.smoothing:\n            plt.plot(steps\\[self.smoothing \\- 1:\\] / self.steps\\_per\\_epoch, self.moving\\_average(self.acc, self.smoothing),\n                     lw\\=1, c\\=\"r\", linestyle\\=\"-\", label\\=\"Train Acc\")\n        \\# Validation curves (scaled to correct x-positions)\n        if self.val\\_losses:\n            plt.plot(np.array(self.val\\_x\\_positions) / self.steps\\_per\\_epoch, self.val\\_losses, c\\=\"b\", linestyle\\=\":\", label\\=\"Val Loss\", lw\\=2)\n        if self.val\\_acc:\n            plt.plot(np.array(self.val\\_x\\_positions) / self.steps\\_per\\_epoch, self.val\\_acc, c\\=\"r\", linestyle\\=\":\", label\\=\"Val Acc\", lw\\=2)\n        plt.xlabel(\"Epochs\", fontsize\\=12)\n        plt.ylabel(\"Loss/Accuracy\", fontsize\\=12)\n        plt.title(f\"Training Progress (Step {self.global\\_step}, Epoch {self.epoch\\_count}, Max Val Acc {self.max\\_acc:.4f})\", fontsize\\=12)\n        plt.xticks(fontsize\\=10)\n        plt.yticks(fontsize\\=10)\n        plt.legend(fontsize\\=10)\n        plt.show()\n## Model selection[#](#model-selection \"Link to this heading\")\n-   Train the neural net and track the loss after every iteration on a validation set\n    -   You can add a callback to the fit version to get info on every epoch\n-   Best model after a few epochs, then starts overfitting\ntrainer \\= pl.Trainer(\n    max\\_epochs\\=15,\n    enable\\_progress\\_bar\\=False,\n    accelerator\\=accelerator,\n    callbacks\\=\\[TrainingBatchPlotCallback()\\] \\# Attach the callback\n)\ntrainer.fit(model, train\\_loader, test\\_loader)\n![../_images/663540e274d5504b79dfc5ade3d5ee5ebe8faccebd81346356d8f46d2fdddc8f.png](https://ml-course.github.io/master/_images/663540e274d5504b79dfc5ade3d5ee5ebe8faccebd81346356d8f46d2fdddc8f.png)\n### Early stopping[#](#early-stopping \"Link to this heading\")\n-   Stop training when the validation loss (or validation accuracy) no longer improves\n-   Loss can be bumpy: use a moving average or wait for \\\\(k\\\\) steps without improvement\n\\# Define early stopping callback\nearly\\_stopping \\= EarlyStopping(\n    monitor\\=\"val\\_loss\", mode\\=\"min\", \\# minimize validation loss\n    patience\\=3)                     \\# Number of epochs with no improvement before stopping\n\\# Update the Trainer to include early stopping as a callback\ntrainer \\= pl.Trainer(\n    max\\_epochs\\=10, accelerator\\=accelerator,\n    callbacks\\=\\[TrainingPlotCallback(), early\\_stopping\\]  \\# Attach the callbacks\n)\n### Regularization and memorization capacity[#](#regularization-and-memorization-capacity \"Link to this heading\")\n-   The number of learnable parameters is called the model _capacity_\n-   A model with more parameters has a higher _memorization capacity_\n    -   Too high capacity causes overfitting, too low causes underfitting\n    -   In the extreme, the training set can be ‘memorized’ in the weights\n-   Smaller models are forced it to learn a compressed representation that generalizes better\n    -   Find the sweet spot: e.g. start with few parameters, increase until overfitting stars.\n-   Example: 256 nodes in first layer, 32 nodes in second layer, similar performance\n    -   Avoid _bottlenecks_: layers so small that information is lost\nself.fc1 = nn.Linear(28 \\* 28, 256)\nself.fc2 = nn.Linear(256, 32)\nself.fc3 = nn.Linear(32, 10)\n#### Weight regularization (weight decay)[#](#weight-regularization-weight-decay \"Link to this heading\")\n-   We can also add weight regularization to our loss function (or invent your own)\n-   L1 regularization: leads to _sparse networks_ with many weights that are 0\n-   L2 regularization: leads to many very small weights\ndef training\\_step(self, batch, batch\\_idx):\n    X\\_batch, y\\_batch \\= batch\n    outputs \\= self(X\\_batch)\n    loss \\= self.criterion(outputs, y\\_batch)\n    l1\\_lambda \\= 1e-5 \\# L1 Regularization\n    l1\\_loss \\= sum(p.abs().sum() for p in self.parameters())\n    l2\\_lambda \\= 1e-4 \\# L2 Regularization\n    l2\\_loss \\= sum((p \\*\\* 2).sum() for p in self.parameters())\n    return loss + l2\\_lambda \\* l2\\_loss  \\# Using L2 only\nAlternative: set `weight_decay` in the optimizer (only for L2 loss)\ndef configure\\_optimizers(self):\n    return optim.RMSprop(self.parameters(), lr\\=0.001, momentum\\=0.0, weight\\_decay\\=1e-4)\n### Dropout[#](#dropout \"Link to this heading\")\n-   Every iteration, randomly set a number of activations \\\\(a\\_i\\\\) to 0\n-   _Dropout rate_ : fraction of the outputs that are zeroed-out (e.g. 0.1 - 0.5)\n    -   Use higher dropout rates for deeper networks\n-   Use higher dropout in early layers, lower dropout later\n    -   Early layers are usually larger, deeper layers need stability\n-   Idea: break up accidental non-significant learned patterns\n-   At test time, nothing is dropped out, but the output values are scaled down by the dropout rate\n    -   Balances out that more units are active than during training\nShow code cell source Hide code cell source\nfig \\= plt.figure(figsize\\=(3\\*fig\\_scale, 3\\*fig\\_scale))\nax \\= fig.gca()\ndraw\\_neural\\_net(ax, \\[2, 3, 1\\], draw\\_bias\\=True, labels\\=True,\n                show\\_activations\\=True, activation\\=True)\n![../_images/42f3c29683ed37223f2b024ff3d1c6824d5d7e483c276a76483c1465f6693acd.png](https://ml-course.github.io/master/_images/42f3c29683ed37223f2b024ff3d1c6824d5d7e483c276a76483c1465f6693acd.png)\n#### Dropout layers[#](#dropout-layers \"Link to this heading\")\n-   Dropout is usually implemented as a special layer\ndef \\_\\_init\\_\\_(self):\n    super(NeuralNetwork, self).\\_\\_init\\_\\_()\n    self.fc1 \\= nn.Linear(28 \\* 28, 512)\n    self.dropout1 \\= nn.Dropout(p\\=0.2)  \\# 20% dropout\n    self.fc2 \\= nn.Linear(512, 512)\n    self.dropout2 \\= nn.Dropout(p\\=0.1)  \\# 10% dropout\n    self.fc3 \\= nn.Linear(512, 10)\ndef forward(self, x):\n    x \\= F.relu(self.fc1(x))\n    x \\= self.dropout1(x)  \\# Apply dropout\n    x \\= F.relu(self.fc2(x))\n    x \\= self.dropout2(x)  \\# Apply dropout\n    return self.fc3(x)\n#### Batch Normalization[#](#batch-normalization \"Link to this heading\")\n-   We’ve seen that scaling the input is important, but what if layer activations become very large?\n    -   Same problems, starting deeper in the network\n-   Batch normalization: normalize the activations of the previous layer within each batch\n    -   Within a batch, set the mean activation close to 0 and the standard deviation close to 1\n        -   Across badges, use exponential moving average of batch-wise mean and variance\n    -   Allows deeper networks less prone to vanishing or exploding gradients\n#### BatchNorm layers[#](#batchnorm-layers \"Link to this heading\")\n-   Batch normalization is also usually implemented as a special layer\ndef \\_\\_init\\_\\_(self):\n    super(NeuralNetwork, self).\\_\\_init\\_\\_()\n    self.fc1 \\= nn.Linear(28 \\* 28, 512)\n    self.bn1 \\= nn.BatchNorm1d(512)  \\# Batch normalization after first layer\n    self.fc2 \\= nn.Linear(512, 265)\n    self.bn2 \\= nn.BatchNorm1d(265)  \\# Batch normalization after second layer\n    self.fc3 \\= nn.Linear(265, 10)\ndef forward(self, x):\n    x \\= x.view(x.size(0), \\-1)  \\# Flatten the image\n    x \\= F.relu(self.bn1(self.fc1(x)))  \\# Apply batch norm after linear layer\n    x \\= F.relu(self.bn2(self.fc2(x)))  \\# Apply batch norm after second layer\n    return self.fc3(x)\n### New model[#](#new-model \"Link to this heading\")\nclass NeuralNetwork(pl.LightningModule):\n    def \\_\\_init\\_\\_(self):\n        super(NeuralNetwork, self).\\_\\_init\\_\\_()\n        self.fc1 \\= nn.Linear(28 \\* 28, 265)\n        self.bn1 \\= nn.BatchNorm1d(265)\n        self.dropout1 \\= nn.Dropout(0.5)\n        self.fc2 \\= nn.Linear(265, 64)\n        self.bn2 \\= nn.BatchNorm1d(64)\n        self.dropout2 \\= nn.Dropout(0.5)\n        self.fc3 \\= nn.Linear(64, 32)\n        self.bn3 \\= nn.BatchNorm1d(32)\n        self.dropout3 \\= nn.Dropout(0.5)\n        self.fc4 \\= nn.Linear(32, 10)\n        self.criterion \\= nn.CrossEntropyLoss(label\\_smoothing\\=0.01)\n        self.accuracy \\= torchmetrics.Accuracy(task\\=\"multiclass\", num\\_classes\\=10)\n    def forward(self, x):\n        x \\= F.relu(self.bn1(self.fc1(x)))\n        x \\= self.dropout1(x)\n        x \\= F.relu(self.bn2(self.fc2(x)))\n        x \\= self.dropout2(x)\n        x \\= F.relu(self.bn3(self.fc3(x)))\n        x \\= self.dropout3(x)\n        x \\= self.fc4(x)\n        return x\n### New model (Sequential API)[#](#new-model-sequential-api \"Link to this heading\")\nmodel \\= nn.Sequential(\n        nn.Linear(28 \\* 28, 265),\n        nn.BatchNorm1d(265),\n        nn.ReLU(),\n        nn.Dropout(0.5),\n        nn.Linear(265, 64),\n        nn.BatchNorm1d(64),\n        nn.ReLU(),\n        nn.Dropout(0.5),\n        nn.Linear(64, 32),\n        nn.BatchNorm1d(32),\n        nn.ReLU(),\n        nn.Dropout(0.5),\n        nn.Linear(32, 10))\nOur model now performs better and is improving.\nclass NeuralNetwork(pl.LightningModule):\n    def \\_\\_init\\_\\_(self):\n        super(NeuralNetwork, self).\\_\\_init\\_\\_()\n        self.fc1 \\= nn.Linear(28 \\* 28, 265)\n        self.bn1 \\= nn.BatchNorm1d(265)\n        self.dropout1 \\= nn.Dropout(0.5)\n        self.fc2 \\= nn.Linear(265, 64)\n        self.bn2 \\= nn.BatchNorm1d(64)\n        self.dropout2 \\= nn.Dropout(0.5)\n        self.fc3 \\= nn.Linear(64, 32)\n        self.bn3 \\= nn.BatchNorm1d(32)\n        self.dropout3 \\= nn.Dropout(0.5)\n        self.fc4 \\= nn.Linear(32, 10)\n        self.accuracy \\= torchmetrics.Accuracy(task\\=\"multiclass\", num\\_classes\\=10)\n        self.criterion \\= nn.CrossEntropyLoss(label\\_smoothing\\=0.01)\n    def forward(self, x):\n        x \\= F.relu(self.bn1(self.fc1(x)))\n        x \\= self.dropout1(x)\n        x \\= F.relu(self.bn2(self.fc2(x)))\n        x \\= self.dropout2(x)\n        x \\= F.relu(self.bn3(self.fc3(x)))\n        x \\= self.dropout3(x)\n        x \\= self.fc4(x)\n        return x\n    def training\\_step(self, batch, batch\\_idx):\n        X\\_batch, y\\_batch \\= batch\n        outputs \\= self(X\\_batch)                 \\# Logits (raw outputs)\n        loss \\= self.criterion(outputs, y\\_batch) \\# Loss\n        preds \\= torch.argmax(outputs, dim\\=1)    \\# Predictions\n        acc \\= self.accuracy(preds, y\\_batch)\n        self.log(\"train\\_loss\", loss, sync\\_dist\\=True)\n        self.log(\"train\\_acc\", acc, sync\\_dist\\=True)\n        return loss\n    def validation\\_step(self, batch, batch\\_idx):\n        X\\_batch, y\\_batch \\= batch\n        outputs \\= self(X\\_batch)\n        loss \\= self.criterion(outputs, y\\_batch)\n        preds \\= torch.argmax(outputs, dim\\=1)\n        acc \\= self.accuracy(preds, y\\_batch)\n        self.log(\"val\\_loss\", loss, sync\\_dist\\=True, on\\_epoch\\=True)\n        self.log(\"val\\_acc\", acc, sync\\_dist\\=True, on\\_epoch\\=True)\n        return loss\n    def on\\_train\\_epoch\\_end(self):\n        avg\\_loss \\= self.trainer.callback\\_metrics\\[\"train\\_loss\"\\].item()\n        avg\\_acc \\= self.trainer.callback\\_metrics\\[\"train\\_acc\"\\].item()\n    def on\\_validation\\_epoch\\_end(self):\n        avg\\_loss \\= self.trainer.callback\\_metrics\\[\"val\\_loss\"\\].item()\n        avg\\_acc \\= self.trainer.callback\\_metrics\\[\"val\\_acc\"\\].item()\n    def configure\\_optimizers(self):\n        return optim.RMSprop(self.parameters(), lr\\=0.001, momentum\\=0.0)\nmodel \\= NeuralNetwork()\ntrainer \\= pl.Trainer(\n    max\\_epochs\\=10,\n    enable\\_progress\\_bar\\=False,\n    accelerator\\=accelerator,\n    callbacks\\=\\[TrainingBatchPlotCallback()\\] \\# Attach the callback\n)\ntrainer.fit(model, train\\_loader, test\\_loader)\n![../_images/172f6478b7699cdb59f49378fea9e10cd922e7a9d6f3635d9fa43d55c0d05e07.png](https://ml-course.github.io/master/_images/172f6478b7699cdb59f49378fea9e10cd922e7a9d6f3635d9fa43d55c0d05e07.png)\n### Other logging tools[#](#other-logging-tools \"Link to this heading\")\n-   There is a lot more tooling to help you build good models\n-   E.g. TensorBoard is easy to integrate and offers a convenient dashboard\nlogger \\= pl.loggers.TensorBoardLogger(\"logs/\", name\\=\"my\\_experiment\")\ntrainer \\= pl.Trainer(max\\_epochs\\=2, logger\\=logger)\ntrainer.fit(lit\\_model, trainloader)\n![ml](https://ml-course.github.io/master/notebooks/images/tensorboard.png)\n## Summary[#](#id2 \"Link to this heading\")\n-   Neural architectures\n-   Training neural nets\n    -   Forward pass: Tensor operations\n    -   Backward pass: Backpropagation\n-   Neural network design:\n    -   Activation functions\n    -   Weight initialization\n    -   Optimizers\n-   Neural networks in practice\n-   Model selection\n    -   Early stopping\n    -   Memorization capacity and information bottleneck\n    -   L1/L2 regularization\n    -   Dropout\n    -   Batch normalization\n[\nprevious\nLecture 5. Data preprocessing\n](https://ml-course.github.io/master/notebooks/05%20-%20Data%20Preprocessing.html \"previous page\")[\nnext\nLecture 7: Convolutional Neural Networks\n](https://ml-course.github.io/master/notebooks/07%20-%20Convolutional%20Neural%20Networks.html \"next page\")\nContents\n-   [Overview](#overview)\n-   [Architecture](#architecture)\n    -   [Basic Architecture](#basic-architecture)\n    -   [More layers](#more-layers)\n    -   [Why layers?](#why-layers)\n    -   [Other architectures](#other-architectures)\n-   [Training Neural Nets](#training-neural-nets)\n    -   [Mini-batch Stochastic Gradient Descent (recap)](#mini-batch-stochastic-gradient-descent-recap)\n    -   [Forward pass](#forward-pass)\n        -   [Tensor operations](#tensor-operations)\n        -   [Element-wise operations](#element-wise-operations)\n    -   [Backward pass (backpropagation)](#backward-pass-backpropagation)\n        -   [Example](#example)\n        -   [Backpropagation (2)](#backpropagation-2)\n        -   [Backpropagation (3)](#backpropagation-3)\n        -   [Summary](#summary)\n-   [Activation functions for hidden layers](#activation-functions-for-hidden-layers)\n    -   [Effect of activation functions on the gradient](#effect-of-activation-functions-on-the-gradient)\n    -   [ReLU vs Tanh](#relu-vs-tanh)\n    -   [Activation functions for output layer](#activation-functions-for-output-layer)\n-   [Weight initialization](#weight-initialization)\n    -   [Weight initialization: transfer learning](#weight-initialization-transfer-learning)\n-   [Optimizers](#optimizers)\n    -   [SGD with learning rate schedules](#sgd-with-learning-rate-schedules)\n    -   [SGD with learning rate schedules](#id1)\n    -   [Momentum](#momentum)\n        -   [Momentum in practice](#momentum-in-practice)\n    -   [Adaptive gradients](#adaptive-gradients)\n    -   [Adam (Adaptive moment estimation)](#adam-adaptive-moment-estimation)\n    -   [SGD Optimizer Zoo](#sgd-optimizer-zoo)\n-   [Neural networks in practice](#neural-networks-in-practice)\n    -   [Preparing the data](#preparing-the-data)\n    -   [Building the network](#building-the-network)\n    -   [Choosing loss, optimizer, metrics](#choosing-loss-optimizer-metrics)\n    -   [Training on GPU](#training-on-gpu)\n    -   [Training loop](#training-loop)\n    -   [`loss.backward()`](#loss-backward)\n    -   [In PyTorch Lightning](#in-pytorch-lightning)\n    -   [Training](#training)\n    -   [Lightning Trainer](#lightning-trainer)\n        -   [Choosing training hyperparameters](#choosing-training-hyperparameters)\n-   [Model selection](#model-selection)\n    -   [Early stopping](#early-stopping)\n    -   [Regularization and memorization capacity](#regularization-and-memorization-capacity)\n        -   [Weight regularization (weight decay)](#weight-regularization-weight-decay)\n    -   [Dropout](#dropout)\n        -   [Dropout layers](#dropout-layers)\n        -   [Batch Normalization](#batch-normalization)\n        -   [BatchNorm layers](#batchnorm-layers)\n    -   [New model](#new-model)\n    -   [New model (Sequential API)](#new-model-sequential-api)\n    -   [Other logging tools](#other-logging-tools)\n-   [Summary](#id2)\nBy Joaquin Vanschoren\n© Copyright 2025. CC0 Licensed - Use as you like. Appropriate credit is very welcome.",
  "timestamp": 1762690841466,
  "title": "Lecture 6. Neural Networks — ML Engineering"
}