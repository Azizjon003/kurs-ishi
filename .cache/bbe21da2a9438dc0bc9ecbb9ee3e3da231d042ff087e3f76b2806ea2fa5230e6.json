{
  "url": "https://eng.libretexts.org/Bookshelves/Data_Science/Principles_of_Data_Science_(OpenStax)/07%3A_Deep_Learning_and_AI_Basics/7.01%3A_Introduction_to_Neural_Networks",
  "markdown": "# 7.1: Introduction to Neural Networks\n*By Libretexts*\n---\n1.  Last updated\n2.  [Save as PDF](https://eng.libretexts.org/@api/deki/pages/118103/pdf/7.1%253A%2bIntroduction%2bto%2bNeural%2bNetworks.pdf \"Export page as a PDF\")\n-   Page ID\n    118103\n-   ![ ](https://biz.libretexts.org/@api/deki/files/5084/girl-160172__340.png \"OpenStax\")\n-   [OpenStax](https://openstax.org/)\n-   [OpenStax](https://openstax.org/)\n##### Learning Objectives\nBy the end of this section, you should be able to:\n-   7.1.1 Define neural networks and discuss the types of problems for which they may be useful.\n-   7.1.2 Summarize the roles of weights and biases in a neural network.\n-   7.1.3 Construct a simple neural network.\nJust imagine what must go on inside the human brain when tasked with recognizing digits, given the varying ways the digits 0-9 can be hand-drawn. Human readers can instantly classify these digits, even from an early age. The goal of neural networks is for a computer algorithm to be able to classify these digits as well as a human.\n### MNIST Database\nThe MNIST (Modified National Institute of Standards and Technology) database is a large dataset of handwritten digits that is frequently used for training various image processing and machine learning systems. This video presentation [by Michael Garris](https://openstax.org/r/michae), senior scientist, provides an overview. You may download the [MNIST dataset files](https://openstax.org/r/yann) directly for further exploration.\nIn this section, we introduce simple models of neural networks and discover how they work. Many technical details are deferred to later sections or are outside the scope of this text.\n## What Is a Neural Network?\nA neural network is a structure made up of components called neurons, which are individual decision-making units that take some number of inputs x1,x2,…,xnFigure 7.2. How the output is determined by the inputs could be quite complex. Moreover, any two _neurons_ may behave quite differently from each other on the same inputs.\n![Diagram of a neural network with a single neuron in the middle. There are three inputs labeled X1, X2, and X3 and one output labeled y.](https://eng.libretexts.org/@api/deki/files/92220/de6f02fdb6f00a1d801a803da7b4f030747bceaf?revision=1)\nFigure 7.2 Single Neuron in a Neural Network\n*Figure 7.2 Single Neuron in a Neural Network*\nThe neural network itself may consist of hundreds, thousands, or even millions of neurons connected to each other in _layers_, or groups of neurons that all receive the same inputs from previous layers and forward signals in aggregate to the next layer. There are always at least two layers, the input layer (containing neurons that accept the initial input data) and output layer (containing the neurons that are used to interpret the answer or give classification information), together with some number of hidden layers (layers between the input and output layers). Figure 7.3 shows a simple neural network diagram for reference.\n![A diagram showing a simple artificial neural network with three input nodes, two hidden layers, and one output node. The input layer consists of three nodes labeled x1, x2, and x3. The first hidden layer has four nodes. The second hidden layer has four nodes. All nodes in each layer are connected to all nodes in the next layer. The output layer consists of one node labeled y.](https://eng.libretexts.org/@api/deki/files/92225/71ceb2475f8c6b8906d94f94cc23de9d7d258f73?revision=1)\nFigure 7.3 Neural Network Diagram. This neural network has four layers, two of which are hidden layers. There are three input neurons and one output neuron. In this example, all nodes in adjacent layers are connected, but some neural network models may not include all such connections (see, for example, convolutional neural networks in Introduction to Deep Learning).\n*Figure 7.3 Neural Network Diagram. This neural network has four layers, two of which are hidden layers. There are three input neurons and one output neuron. In this example, all nodes in adjacent layers are connected, but some neural network models may not include all such connections (see, for example, convolutional neural networks in Introduction to Deep Learning).*\nThe main purpose of a neural network is to classify complex data. Problems for which neural networks are especially well suited include the following:\n-   Image recognition, including facial recognition, identifying handwritten letters and symbols, and classifying parts of images. This is a huge area of innovation, with powerful tools such as TensorFlow developed by Google and [PyTorch](https://openstax.org/r/pytorch) developed by Meta.\n-   Speech recognition, such as Google’s [Cloud Speech-to-Text](https://openstax.org/r/speech) service, offers accurate transcription of speech and translation for various languages.\n-   Recommendation systems, which are used to serve ads online based on each user’s browsing habits, have been developed and used by Amazon, Meta, Netflix, and many other large companies to reach target markets.\n-   Anomaly detection has been developed to aid in fraud detection, data security, and error analysis/correction by finding outliers in large, complex datasets. An example is Microsoft’s [Azure Anomaly Detector](https://openstax.org/r/azure), which can detect anomalies in time series data.\n-   Autonomous vehicles and robotics, including Tesla’s Autopilot technology, are becoming more and more prominent as automation alleviates some of the routine aspects of daily life and leads to increased efficiencies in business, manufacturing, and transportation.\n-   Generative art, examples of which include visual art, music, video, and poetry, leverage vast stores of human creative output to produce novel variations.\n-   Predictive text, including natural language processing models such as ChatGPT (see Convolutional Neural Networks).\n### TensorFlow\nIf you want to get your feet wet with neural networks, check out this interactive web-based neural network tool, called [TensorFlow Playground](https://openstax.org/r/playground), which uses TensorFlow to train and update outputs in real time. There, you can choose a dataset from a list, adjust the number of hidden layers and neurons per layer by clicking the plus (+) and minus (\\-\\-) buttons, and adjust the learning rate, choice of activation function, and other parameters (topics that we will learn more about in the rest of the chapter). Then click the “play” button to start training the model and watch as it learns how to classify the points in your chosen dataset! If you want to start over, just click “reset” and start from scratch!\n### Neurons, Weights, Biases, and Activation Functions\nThe way neurons work in a neural network is conjectured to be similar, or _analogous_, to how neurons work in the brain. The input of the neural network is fed into the neurons of the input layer. Each neuron then processes it and produces an output, which is in turn pushed to the neurons in the next layer. Individual neurons only send signals, or _activate_, if they receive the appropriate input required to activate. (Activation is the process of sending an output signal after having received appropriate input signals.) After passing through some number of hidden layers, the output of the last hidden layer feeds into the output layer. Lastly, the output of this final layer is interpreted based on the nature of the problem. If there is a single output neuron, then the interpretation could be _true_ if that neuron is activated or _false_ if not. For neural networks used to classify input into various classes (e.g., number recognition), there is usually one output neuron per class. The one that is _most_ activated would indicate the classification, as shown in Figure 7.4.\n![A neural network diagram with four output neurons, labeled A (0.63), B (0.81), C (0.32), and D (0.09). As the highest activation level, B is highlighted in green and the output of the neural network is B.](https://eng.libretexts.org/@api/deki/files/92224/a2b6bcdb38bc114b060007e6bb8e5d2f6e391997?revision=1)\nFigure 7.4 Output Neurons. In this figure, there are four output neurons, labeled A, B, C, and D. Since B has the highest activation level, the output of the neural network is B.\n*Figure 7.4 Output Neurons. In this figure, there are four output neurons, labeled A, B, C, and D. Since B has the highest activation level, the output of the neural network is B.*\nEach connection from one neuron to another has two parameters associated with it. The weight is a value wFigure 7.6.)\nConsider the simplest case of all, a neuron with single input xx and output yy. Then the flow of signal through the neuron follows the formula y\\=f(wx+b)y\\=f(wx+b). For example, suppose that the input value is x\\=0.87x\\=0.87, with weight w\\=0.53w\\=0.53 and bias b\\=−0.12b\\=−0.12. Then the signal would first be combined as wx+b\\=(0.53)(0.87)+(−0.12)\\=0.3411wx+b\\=(0.53)(0.87)+(−0.12)\\=0.3411. This value would be fed as input to the activation function to produce y\\=f(0.3411)y\\=f(0.3411). In the next section, we will discuss various activation functions used in neural networks.\nWhen there are multiple inputs, x1,x2,…,xnx1,x2,…,xn, each will be affected by its own weight. Typically, bias is thought of as a property of the neuron itself and so bias affects all the inputs in the same way. To be more precise, first, the inputs are multiplied by their individual weights, the result is summed, and then the bias is added. Finally, the activation function is applied to obtain the output signal.\ny\\=f(w1x1+w2x2+…+wnxn+b)\\=f((∑i\\=1nwixi)+b)y\\=f(w1x1+w2x2+…+wnxn+b)\\=f((∑i\\=1nwixi)+b)\nThe weights and biases are parameters that the neural network learns during the training process, a topic we will explain in Backpropagation.\nA typical neural network may have hundreds or thousands of inputs for each neuron, and so the equation can be difficult to work with. It would be more convenient to regard all the inputs x1,x2,…,xnx1,x2,…,xn as parts of a single mathematical structure, called a vector. A **vector** is simply an ordered list of numbers, that is, x\\=(x1,x2,…,xn)x\\=(x1,x2,…,xn). (Note: In this text we use boldface letters to represent vectors. Also, the components of a vector are listed within a set of parentheses. Some texts vary on these notational details.) The number of components in a vector is called its dimension, so for example the vector (5,0,−2,1.2,π)(5,0,−2,1.2,π) has dimension 5. Certain arithmetic operations are defined on vectors.\n-   Vectors of the same dimension can be added: If x\\=(x1,x2,…,xn)x\\=(x1,x2,…,xn) and y\\=(y1,y2,…,yn)y\\=(y1,y2,…,yn), then x+y\\=(x1+y1,x2+y2,…,xn+yn)x+y\\=(x1+y1,x2+y2,…,xn+yn).\n-   Any real number can be multiplied to a vector: kx\\=k(x1,x2,…,xn)\\=(kx1,kx2,…,kxn)kx\\=k(x1,x2,…,xn)\\=(kx1,kx2,…,kxn).\n-   The **dot product** of two vectors of the same dimension results in a real number (not a vector) and is defined by:\n    x·y\\=(x1,x2,…,xn)·(y1,y2,…,yn)\\=∑i\\=1nxiyi\\=x1y1+x2y2+x3y3+…+xnynx·y\\=(x1,x2,…,xn)·(y1,y2,…,yn)\\=∑i\\=1nxiyi\\=x1y1+x2y2+x3y3+…+xnyn\nIf the inputs and weights are regarded as vectors, x\\=(x1,x2,…,xn)x\\=(x1,x2,…,xn) and w\\=(w1,w2,…,wn)w\\=(w1,w2,…,wn), respectively, then the formula may be re-expressed more concisely as:\ny\\=f(w·x+b)y\\=f(w·x+b)\nFor example, if w\\=(0.3,−0.1,0.9)w\\=(0.3,−0.1,0.9), x\\=(1.2,0.4,−0.6)x\\=(1.2,0.4,−0.6), and b\\=0.1b\\=0.1, then\nw·x+b\\=(0.3)(1.2)+(−0.1)(0.4)+(0.9)(−0.6)+0.1\\=−0.12w·x+b\\=(0.3)(1.2)+(−0.1)(0.4)+(0.9)(−0.6)+0.1\\=−0.12\nSo in this example, the output would be y\\=f(−0.12)y\\=f(−0.12), the exact value of which depends on which activation function f(x)f(x) is chosen.\n### Types of Activation Functions\nActivation functions come in many types. Here are just a few of the most common activation functions.\n1.  Step function, f(x)\\={0,ifx<c1,ifx≥cf(x)\\={0,ifx<c1,ifx≥c. The value of cc serves as a threshold. The neuron only activates when the input is at least equal to a parameter cc.\n2.  Sigmoid function, σ(x)\\=11+e−xClassification Using Machine Learning). Output values tend to be close to 0 when xx is negative and close to 1 when xx is positive, with a smooth transition in between.\n3.  Hyperbolic tangent (tanh) function, tanh x\\=ex−e−xex+e−xtanh x\\=ex−e−xex+e−x. Output values have the same sign as xx, with a smooth transition through 0.\n4.  Rectified linear unit (ReLU) function, ReLU(x)\\=max(0,x)ReLU(x)\\=max(0,x). The ReLU function is 0 for negative xx values and equal to the input when xx is positive.\n5.  Leaky ReLU function, LReLU(x)\\=max(cx,x)LReLU(x)\\=max(cx,x), for some small positive parameter cc. Leaky ReLU acts much like ReLU except that the values get progressively more negative when xx gets more negative. Often, the optimal “leakiness” parameter cc is determined during the training phase.\n6.  Softplus function, f(x)\\=ln(1+ex)f(x)\\=ln(1+ex), which is a smoothed version of the ReLU function.\nFigure 7.5 shows the graphs of the listed functions. A key feature of activation functions is that they are nonlinear, meaning that they are not just straight lines, f(x)\\=mx+bf(x)\\=mx+b.\nAnother activation function that is important in neural networks, softmax, takes a vector of real number values and yields a vector of values scaled into the interval between 0 and 1, which can be interpreted as discrete probability distribution. (Recall from Discrete and Continuous Probability Distributions that discrete probability distributions provide measures of probability or likelihood that each of a finite number of values might occur.) The formula for softmax is:\nσ(x1,x2,…,xn)\\=(ex1D,ex2D,…,exnD)σ(x1,x2,…,xn)\\=(ex1D,ex2D,…,exnD)\nwhere D\\=ex1+ex2+…+exnBackpropagation.\n![A grid of six plots showing different activation functions commonly used in neural networks. Each plot displays a curve representing the function's output values across a range of input values. Top, from left to right, the graphs are labeled (a) step, (b) sigmoid, and (c) tanh. Bottom, from left to right, the graphs are labeled (d) ReLU, (e) Leaky ReLU, and (f) softplus. X and Y axes are -2 to 2 with a blue line in each graph representing the function.](https://eng.libretexts.org/@api/deki/files/92221/a6a801e1bf821576ab221cafc95736f1fd3866f1?revision=1)\nFigure 7.5 Graphs of Activation Functions. Top, from left to right, (a) step, (b) sigmoid, and (c) hyperbolic tangent (tanh) functions. Bottom, from left to right, (d) ReLU, (e) LReLU, and (f) softplus functions.\n*Figure 7.5 Graphs of Activation Functions. Top, from left to right, (a) step, (b) sigmoid, and (c) hyperbolic tangent (tanh) functions. Bottom, from left to right, (d) ReLU, (e) LReLU, and (f) softplus functions.*\nA simple neuron has four inputs, x1,x2,x3,x4x1,x2,x3,x4, and one output, yy. The weights of the four inputs are w1\\=0.34w1\\=0.34, w2\\=0.07w2\\=0.07, w3\\=0.59w3\\=0.59, and w4\\=−0.21w4\\=−0.21. The bias of the neuron is b\\=0.19b\\=0.19. Find the output in each of the following cases.\n1.  (x1,x2,x3,x4)\\=(1,0,0,1)(x1,x2,x3,x4)\\=(1,0,0,1). Activation function: ReLU.\n2.  (x1,x2,x3,x4)\\=(1,0,0,1)(x1,x2,x3,x4)\\=(1,0,0,1). Activation function: sigmoid.\n3.  (x1,x2,x3,x4)\\=(1,0,0,1)(x1,x2,x3,x4)\\=(1,0,0,1). Activation function: step, f(x)\\={0,ifx<0.51,ifx≥0\\. 5f(x)\\={0,ifx<0.51,ifx≥0\\. 5.\n4.  (x1,x2,x3,x4)\\=(0,2.3,−1.6,0.8)(x1,x2,x3,x4)\\=(0,2.3,−1.6,0.8). Activation function: softplus.\n**Answer**\nSetup for each part:\ny \\= f ( w · x + b ) \\= f ( w 1 x 1 + w 2 x 2 + w 3 x 3 + w 4 x 4 + b ) \\= f ( 0.34 x 1 + 0.07 x 2 + 0.59 x 3 − 0.21 x 4 + 0.19 ) y \\= f ( w · x + b ) \\= f ( w 1 x 1 + w 2 x 2 + w 3 x 3 + w 4 x 4 + b ) \\= f ( 0.34 x 1 + 0.07 x 2 + 0.59 x 3 − 0.21 x 4 + 0.19 )\n1.  ReLU(0.34(1)+0.07(0)+0.59(0)−0.21(1)+0.19)\\=ReLU(0.32)\\=0.32ReLU(0.34(1)+0.07(0)+0.59(0)−0.21(1)+0.19)\\=ReLU(0.32)\\=0.32, since 0.32\\>00.32\\>0\n2.  σ(0.32)\\=11+e−0.32\\=0.58σ(0.32)\\=11+e−0.32\\=0.58 (Note, 0.32 was already computed in part a.)\n3.  f(0.32)\\=0f(0.32)\\=0, since 0.32<0.50.32<0.5 (Note, 0.32 was already computed in part a.)\n4.  f(0.34(0)+0.07(2.3)+0.59(−1.6)−0.21(0.8)+0.19)\\=f(−0.761)\\=ln(1+e−0.761)\\=−0.761f(0.34(0)+0.07(2.3)+0.59(−1.6)−0.21(0.8)+0.19)\\=f(−0.761)\\=ln(1+e−0.761)\\=−0.761\nIn the next section, we explore neural networks that use the simplest of the activation functions, the step function.\n## Perceptrons\nAlthough the idea of using structures modeled after biological neurons had been around prior to the development of what we would now consider neural networks, a significant breakthrough occurred in the middle of the 20th century. In 1957, Frank Rosenblatt developed the perceptron, a type of single-layer neural network designed for binary classification tasks—in other words, a neural network whose output could be “true” or “false” (see Figure 7.7). After initial successes, progress in AI faced challenges due to limited capabilities of AI programs and the lack of computing power, leading to a period of minimal progress, called the “Winter of AI.” Despite these setbacks, new developments occurred in the 1980s leading to the multilayer perceptron (MLP), which serves as the basic paradigm for neural networks having multiple hidden layers.\nThe single-layer perceptron learns weights through a simple learning rule, known as the perceptron learning rule.\n1.  Initialize the weights (ww) and bias (bb) to small random values.\n2.  For each input (xx) in the training set,\n    1.  Compute predictions y^\\=f(w·x+b)y^\\=f(w·x+b), where ff is the step function,\n        f(x)\\={0,ifx<01,ifx≥0f(x)\\={0,ifx<01,ifx≥0\n    2.  Find the error E\\=y−y^E\\=y−y^, where yy is the true or desired value corresponding to xx. Note that EE can be positive or negative. Positive EE implies the weights and/or bias is too low and needs to be raised. Negative EE implies the opposite.\n    3.  Update the weights and biases according to the following formulas, where hh is a small positive constant that controls the learning rate. Often the change of weights and biases will be small, not immediately causing a large effect on input. However, with repeated training, the perceptron will _learn_ the appropriate values of ww and bb to achieve the desired output.\n        w←w+hExw←w+hEx\n        b←b+hEb←b+hE\nFor example, suppose a perception with three neurons currently has weights w\\=(0.5,0.7,0.1)w\\=(0.5,0.7,0.1) and bias b\\=−0.9b\\=−0.9. On input x\\=(0.6,1,0)x\\=(0.6,1,0), we get:\ny^\\=f((0.5)(0.6)+(0.7)(1)+(0.1)(0)−0.9)\\=f(0.1)\\=1y^\\=f((0.5)(0.6)+(0.7)(1)+(0.1)(0)−0.9)\\=f(0.1)\\=1\nSuppose that the true output should have been y\\=0y\\=0. So there’s an error of E\\=0−1\\=−1E\\=0−1\\=−1. If the learning rate is h\\=0.1h\\=0.1, then the weights and bias will be updated as follows:\nw←w+hEx\\=(0.5+(0.1)(−1)(0.6),0.7+(0.1)(−1)(1),0.1+(0.1)(−1)(0))\\=(0.44,0.6,0.1)w←w+hEx\\=(0.5+(0.1)(−1)(0.6),0.7+(0.1)(−1)(1),0.1+(0.1)(−1)(0))\\=(0.44,0.6,0.1)\nb←b+hE\\=−0.9+(0.1)(−1)\\=−1.0b←b+hE\\=−0.9+(0.1)(−1)\\=−1.0\nOn the same input, the perceptron now has a value of:\ny^\\=f((0.44)(0.6)+(0.6)(1)+(0.1)(0)−1.0)\\=f(−0.136)\\=0y^\\=f((0.44)(0.6)+(0.6)(1)+(0.1)(0)−1.0)\\=f(−0.136)\\=0\nIn this simple example, the value y^y^ changed from 1 to 0, eliminating the error. However, there is no guarantee that the perceptron will classify all input without error, regardless of the number of training steps that are taken.\nSuppose the next data point in the training set is x\\=(0,0.5,1)x\\=(0,0.5,1), and suppose that the correct classification for this point is y\\=1y\\=1. With current weights w\\=(0.44,0.6,0.1)w\\=(0.44,0.6,0.1) and bias b\\=−1b\\=−1, use the perceptron learning rule with h\\=0.1h\\=0.1 to update the weights and bias. Is there an improvement in classifying this data point?\n**Answer**\ny ^ \\= f ( ( 0.44 ) ( 0 ) + ( 0.6 ) ( 0.5 ) + ( 0.1 ) ( 1 ) − 1.0 ) \\= f ( − 0.6 ) \\= 0 y ^ \\= f ( ( 0.44 ) ( 0 ) + ( 0.6 ) ( 0.5 ) + ( 0.1 ) ( 1 ) − 1.0 ) \\= f ( − 0.6 ) \\= 0\nError: E\\=1−0\\=1E\\=1−0\\=1.\nw ← w + h E x \\= ( 0.44 + ( 0.1 ) ( 1 ) ( 0 ) , 0.6 + ( 0.1 ) ( 1 ) ( 0.5 ) , 0.1 + ( 0.1 ) ( 1 ) ( 1 ) ) \\= ( 0.44 , 0.65 , 0.2 ) w ← w + h E x \\= ( 0.44 + ( 0.1 ) ( 1 ) ( 0 ) , 0.6 + ( 0.1 ) ( 1 ) ( 0.5 ) , 0.1 + ( 0.1 ) ( 1 ) ( 1 ) ) \\= ( 0.44 , 0.65 , 0.2 )\nb ← b + h E \\= − 1 + ( 0.1 ) ( 1 ) \\= − 0.9 b ← b + h E \\= − 1 + ( 0.1 ) ( 1 ) \\= − 0.9\nNow we obtain y^\\=f((0.44)(0)+(0.65)(0.5)+(0.2)(1)−0.9)\\=f(−0.375)y^\\=f((0.44)(0)+(0.65)(0.5)+(0.2)(1)−0.9)\\=f(−0.375), which still misclassifies the point as 0, so no improvement in accuracy. (However, at least the argument has increased from −0.6−0.6 to −0.375−0.375, so some progress has been made in training. In practice, many rounds of training may be necessary to achieve accurate results, with only incremental progress in each round.)\n## Building a Simple Neural Network\nIn What Is Data Science?, **Example 1.7**, we looked at the Iris Flower dataset to become familiar with dataset formats and structures. In this section, we will use the Iris dataset, [iris.csv](https://openstax.org/r/datach7), to create a neural network with four inputs and one output neuron (essentially, a simple perceptron) to classify irises as either _setosa_ or _not setosa_, according to four features: sepal length (SL), sepal width (SW), petal length (PL), and petal width (PW), all of which are measured in centimeters. The Iris dataset `iris.csv` has 150 samples of irises, consisting of 50 of each species, _setosa_, _versicolor_, and _virginica_. For the purposes of this example, the first 100 rows of data were selected for training and testing, using 75% of the data for training and the remaining 25% for testing. (Recall from Decision-Making Using Machine Learning Basics that it is good practice to use about 70%–80% of the data for training, with the rest used for testing.) The network will be trained using the perceptron learning rule, but instead of using the step function, the activation function will be the hyperbolic tangent, tanhx\\=ex−e−xex+e−xtanhx\\=ex−e−xex+e−x, which is chosen simply for illustration purposes.\nThe iris database was collected and used by Sir R. A. Fisher for his 1936 paper “The Use of Multiple Measurements in Taxonomic Problems.” This work became a landmark study in the use of multivariate data in classification problems and frequently makes an appearance in data science as a convenient test case for machine learning and neural network algorithms.\nThe model for the neural network is shown in Figure 7.6.\n![A neural network diagram with four inputs labeled X1, X2, X3, and X4. Lines from the inputs to a circle with b in the center (representing bias) are labeled W1, W2, W3, and W4 to represent weights. The bias points to a rectangle labeled tanh and the output is y.](https://eng.libretexts.org/@api/deki/files/92223/ed003c2323811e656cb05dc80661318d81efd43f?revision=1)\nFigure 7.6 Simple Perceptron Neural Network Model. Here, x1,x2,x3,x4x1,x2,x3,x4 are the inputs, w1,w2,w3,w4w1,w2,w3,w4 represent the weights, and bb represents the bias. The output, or response, is yy.\n*Figure 7.6 Simple Perceptron Neural Network Model. Here, x1,x2,x3,x4x1,x2,x3,x4 are the inputs, w1,w2,w3,w4w1,w2,w3,w4 represent the weights, and bb represents the bias. The output, or response, is yy.*\nThe inputs (x1,x2,x3,x4)(x1,x2,x3,x4) will take the values of SL, SW, PL, and PW, respectively. There are four corresponding weights (w1,w2,w3,w4)(w1,w2,w3,w4) and one bias value bb, which are applied to the input layer values, and then the result is fed into the tanh function to obtain the output yy. Thus, the formula used to produce the predicted yy values, denoted by y^y^, is:\ny^\\=tanh(w1x1+w2x2+w3x3+w4x4+b)y^\\=tanh(w1x1+w2x2+w3x3+w4x4+b)\nIn this very simplistic model, we will interpret the value of yy as follows:\n-   If y^\\>0y^\\>0, classify the input as _setosa_.\n-   If y^<0y^<0, classify the input as _not setosa_.\nAs for classifying error, we need to pick concrete target values to compare our outputs with. Let’s consider the target value for classifying _setosa_ as 11, while the target for classifying not _setosa_ is −1−1. This is reasonable as the values of tanh(x)tanh(x) approach 11 as xx increases to positive infinity and \\-1\\-1 as xx decreases to negative infinity.\nFor example, if after training, the ideal weights were found to be (w1,w2,w3,w4)\\=(−0.2,0.6,−0.9,0.1)(w1,w2,w3,w4)\\=(−0.2,0.6,−0.9,0.1), and the bias is b\\=0.5b\\=0.5, then the response on an input (x1,x2,x3,x4)\\=(5.1,3.5,1.4,0.2)(x1,x2,x3,x4)\\=(5.1,3.5,1.4,0.2) would be:\ny^\\=tanh((−0.2)(5.1)+(0.6)(3.5)+(−0.9)(1.4)+(0.1)(0.2)+0.5)\\=tanh(0.34)\\=0.327y^\\=tanh((−0.2)(5.1)+(0.6)(3.5)+(−0.9)(1.4)+(0.1)(0.2)+0.5)\\=tanh(0.34)\\=0.327\nSince y^\\>0y^\\>0, classify the input as _setosa_. Of course, the natural question would be, how do we obtain those particular weights and bias values in the first place? This complicated task is best suited for a computer. We will use Python to train the perceptron.\nThe Python library [sklearn.datasets](https://openstax.org/r/scikit2) has a function, `load_iris`, that automatically loads the Iris dataset, so you should not need to import the dataset [iris.csv](https://openstax.org/r/datach7). Other modules that are imported from `sklearn` are `Perceptron`, used to build the perceptron model, `train_test_split`, used to randomly split a dataset into a testing set and training set. Here, we will use a split of 75% train and 25% test, which is handled by the parameter `test_size=0.25`. We will also renormalize the data so that all features contribute equally to the model. The way to do this in Python is by using `StandardScalar`, which renormalizes the features of the dataset so that the mean is 0 and standard deviation is 1. Finally, `accuracy_score` is used to compute the accuracy of the classification.\n    from sklearn.datasets import load\\_iris\n    from sklearn.linear\\_model import Perceptron\n    from sklearn.model\\_selection import train\\_test\\_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import accuracy\\_score\n    iris = load\\_iris()\n    X = iris.data\\[:100\\]\n    y = iris.target\\[:100\\]\n    X\\_train, X\\_test, y\\_train, y\\_test = train\\_test\\_split(X, y, test\\_size=0.25)\n    scaler = StandardScaler()\n    X\\_train\\_scaled = scaler.fit\\_transform(X\\_train)\n    X\\_test\\_scaled = scaler.transform(X\\_test)\n    perceptron = Perceptron()\n    perceptron.fit(X\\_train\\_scaled, y\\_train)\n    y\\_pred = perceptron.predict(X\\_test\\_scaled)\n    accuracy = accuracy\\_score(y\\_test, y\\_pred)\n    print(\"Accuracy:\", accuracy)\nThe resulting output will look like this:\nAccuracy: 1.0\nBecause the dataset was relatively small (100 rows) and well-structured, the perceptron model was able to classify the data with 100% accuracy (“Accuracy: 1.0”). In the real world, when datasets are larger and less regular, we do not expect 100% accuracy. There could also be significant overfitting in the model, which would lead to high variance when classifying data not found in the original dataset.\nHere is how to use the perceptron model we just created to classify new data (outside of the original dataset):\n    new\\_inputs = \\[\\[5.1, 3.5, 1.4, 0.2\\],\n          \\[6.3, 2.9, 5.6, 1.8\\]\n          \\]\n    new\\_inputs\\_scaled = scaler.transform(new\\_inputs)\n    predictions = perceptron.predict(new\\_inputs\\_scaled)\n    for i, prediction in enumerate(predictions):\n     print(f\"Input {i+1} prediction: {'Setosa' if prediction == 0 else 'Not Setosa'}\")\nThe resulting output will look like this:\nInput 1 prediction: Setosa\nInput 2 prediction: Not Setosa",
  "timestamp": 1762690819154,
  "title": "7.1: Introduction to Neural Networks"
}